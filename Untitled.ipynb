{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 14)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143\n",
       "2     87\n",
       "1     50\n",
       "3     23\n",
       "Name: cp, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    165\n",
       "0    138\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.get_dummies(data['cp'], prefix = \"cp\")\n",
    "b = pd.get_dummies(data['thal'], prefix = \"thal\")\n",
    "c = pd.get_dummies(data['slope'], prefix = \"slope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /home/daniel/.local/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/daniel/.local/lib/python3.6/site-packages (from sklearn) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/daniel/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.15.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /home/daniel/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from networks.NeuralNet import FullyConnectedNet\n",
    "from networks.cnn import CNN\n",
    "from data import mnist\n",
    "from layers import *\n",
    "import trainer\n",
    "from PCA.pca import *\n",
    "from PCA.plot_utils import *\n",
    "import matplotlib.image as mpimg\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy\n",
    "from scipy import io\n",
    "import sys\n",
    "!{sys.executable} -m pip install --user sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>...</th>\n",
       "      <th>cp_1</th>\n",
       "      <th>cp_2</th>\n",
       "      <th>cp_3</th>\n",
       "      <th>thal_0</th>\n",
       "      <th>thal_1</th>\n",
       "      <th>thal_2</th>\n",
       "      <th>thal_3</th>\n",
       "      <th>slope_0</th>\n",
       "      <th>slope_1</th>\n",
       "      <th>slope_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6   \n",
       "\n",
       "    ...     cp_1  cp_2  cp_3  thal_0  thal_1  thal_2  thal_3  slope_0  \\\n",
       "0   ...        0     0     1       0       1       0       0        1   \n",
       "1   ...        0     1     0       0       0       1       0        1   \n",
       "2   ...        1     0     0       0       0       1       0        0   \n",
       "3   ...        1     0     0       0       0       1       0        0   \n",
       "4   ...        0     0     0       0       0       1       0        0   \n",
       "\n",
       "   slope_1  slope_2  \n",
       "0        0        0  \n",
       "1        0        0  \n",
       "2        0        1  \n",
       "3        0        1  \n",
       "4        0        1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [data, a, b, c]\n",
    "df = pd.concat(frames, axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>ca</th>\n",
       "      <th>...</th>\n",
       "      <th>cp_1</th>\n",
       "      <th>cp_2</th>\n",
       "      <th>cp_3</th>\n",
       "      <th>thal_0</th>\n",
       "      <th>thal_1</th>\n",
       "      <th>thal_2</th>\n",
       "      <th>thal_3</th>\n",
       "      <th>slope_0</th>\n",
       "      <th>slope_1</th>\n",
       "      <th>slope_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  ca  \\\n",
       "0   63    1       145   233    1        0      150      0      2.3   0   \n",
       "1   37    1       130   250    0        1      187      0      3.5   0   \n",
       "2   41    0       130   204    0        0      172      0      1.4   0   \n",
       "3   56    1       120   236    0        1      178      0      0.8   0   \n",
       "4   57    0       120   354    0        1      163      1      0.6   0   \n",
       "\n",
       "    ...     cp_1  cp_2  cp_3  thal_0  thal_1  thal_2  thal_3  slope_0  \\\n",
       "0   ...        0     0     1       0       1       0       0        1   \n",
       "1   ...        0     1     0       0       0       1       0        1   \n",
       "2   ...        1     0     0       0       0       1       0        0   \n",
       "3   ...        1     0     0       0       0       1       0        0   \n",
       "4   ...        0     0     0       0       0       1       0        0   \n",
       "\n",
       "   slope_1  slope_2  \n",
       "0        0        0  \n",
       "1        0        0  \n",
       "2        0        1  \n",
       "3        0        1  \n",
       "4        0        1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns = ['cp', 'thal', 'slope'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:16: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  app.launch_new_instance()\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:19: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(242, 13)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train = train['target']\n",
    "#X_train = train.drop(['target'],  axis =1)\n",
    "#y_test = test['target']\n",
    "#X_test = test.drop(['target'],  axis =1)\n",
    "\n",
    "y = data['target']\n",
    "x = data.drop(['target'],  axis =1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y, test_size = 0.2,random_state = 5)\n",
    "\n",
    "X_train, _, _ = feature_normalize(X_train)\n",
    "X_test, _, _ = feature_normalize(X_test)\n",
    "\n",
    "data = {}\n",
    "data['X_train'] = X_train.as_matrix()\n",
    "data['y_train'] = y_train.values\n",
    "data['y_val'] = y_test.values\n",
    "data['X_val'] = X_test.as_matrix()\n",
    "\n",
    "data['X_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07030624, -1.50674161, -0.97227182, ..., -0.65683133,\n",
       "        -0.69043052, -0.49747423],\n",
       "       [-1.2729129 , -1.50674161, -0.97227182, ..., -0.65683133,\n",
       "        -0.69043052,  1.15168692],\n",
       "       [-0.15356362,  0.6636838 ,  0.97227182, ...,  0.98186126,\n",
       "         2.31108179, -0.49747423],\n",
       "       ...,\n",
       "       [-0.93710811, -1.50674161,  0.        , ...,  0.98186126,\n",
       "        -0.69043052, -0.49747423],\n",
       "       [-1.49678275,  0.6636838 , -0.97227182, ...,  0.98186126,\n",
       "        -0.69043052,  1.15168692],\n",
       "       [ 0.51804595,  0.6636838 , -0.97227182, ..., -0.65683133,\n",
       "         0.31007359,  1.15168692]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer set to <function adam at 0x7fcfa41d7730>.\n",
      "(Iteration 1 / 12100) loss: 0.693147\n",
      "(Epoch 1 / 5000) train acc: 0.553719; val_acc: 0.508197\n",
      "F1: 0.673913043478261 \t Precision 0.5081967213114754 \t recall 1.0\n",
      "(Iteration 11 / 12100) loss: 0.552499\n",
      "(Iteration 21 / 12100) loss: 0.396308\n",
      "(Iteration 31 / 12100) loss: 0.340781\n",
      "(Iteration 41 / 12100) loss: 0.378945\n",
      "(Iteration 51 / 12100) loss: 0.338924\n",
      "(Iteration 61 / 12100) loss: 0.223029\n",
      "(Iteration 71 / 12100) loss: 0.237306\n",
      "(Iteration 81 / 12100) loss: 0.211510\n",
      "(Iteration 91 / 12100) loss: 0.262387\n",
      "(Iteration 101 / 12100) loss: 0.268268\n",
      "(Iteration 111 / 12100) loss: 0.408915\n",
      "(Iteration 121 / 12100) loss: 0.252494\n",
      "(Iteration 131 / 12100) loss: 0.365524\n",
      "(Iteration 141 / 12100) loss: 0.224530\n",
      "(Iteration 151 / 12100) loss: 0.221100\n",
      "(Iteration 161 / 12100) loss: 0.294677\n",
      "(Iteration 171 / 12100) loss: 0.186524\n",
      "(Iteration 181 / 12100) loss: 0.195827\n",
      "(Iteration 191 / 12100) loss: 0.184089\n",
      "(Iteration 201 / 12100) loss: 0.176025\n",
      "(Iteration 211 / 12100) loss: 0.127063\n",
      "(Iteration 221 / 12100) loss: 0.136273\n",
      "(Iteration 231 / 12100) loss: 0.211988\n",
      "(Iteration 241 / 12100) loss: 0.237483\n",
      "(Iteration 251 / 12100) loss: 0.239600\n",
      "(Iteration 261 / 12100) loss: 0.213707\n",
      "(Iteration 271 / 12100) loss: 0.164909\n",
      "(Iteration 281 / 12100) loss: 0.138213\n",
      "(Iteration 291 / 12100) loss: 0.151616\n",
      "(Iteration 301 / 12100) loss: 0.170572\n",
      "(Iteration 311 / 12100) loss: 0.299944\n",
      "(Iteration 321 / 12100) loss: 0.185607\n",
      "(Iteration 331 / 12100) loss: 0.213856\n",
      "(Iteration 341 / 12100) loss: 0.225651\n",
      "(Iteration 351 / 12100) loss: 0.164127\n",
      "(Iteration 361 / 12100) loss: 0.118964\n",
      "(Iteration 371 / 12100) loss: 0.168551\n",
      "(Iteration 381 / 12100) loss: 0.218431\n",
      "(Iteration 391 / 12100) loss: 0.178433\n",
      "(Iteration 401 / 12100) loss: 0.109647\n",
      "(Iteration 411 / 12100) loss: 0.089703\n",
      "(Iteration 421 / 12100) loss: 0.206647\n",
      "(Iteration 431 / 12100) loss: 0.160137\n",
      "(Iteration 441 / 12100) loss: 0.149751\n",
      "(Iteration 451 / 12100) loss: 0.121054\n",
      "(Iteration 461 / 12100) loss: 0.181607\n",
      "(Iteration 471 / 12100) loss: 0.210862\n",
      "(Iteration 481 / 12100) loss: 0.195495\n",
      "(Iteration 491 / 12100) loss: 0.159500\n",
      "(Iteration 501 / 12100) loss: 0.123502\n",
      "(Epoch 1 / 5000) train acc: 0.818182; val_acc: 0.754098\n",
      "F1: 0.8051948051948052 \t Precision 0.6739130434782609 \t recall 1.0\n",
      "(Iteration 511 / 12100) loss: 0.140621\n",
      "(Iteration 521 / 12100) loss: 0.232384\n",
      "(Iteration 531 / 12100) loss: 0.142072\n",
      "(Iteration 541 / 12100) loss: 0.093482\n",
      "(Iteration 551 / 12100) loss: 0.258964\n",
      "(Iteration 561 / 12100) loss: 0.153927\n",
      "(Iteration 571 / 12100) loss: 0.143009\n",
      "(Iteration 581 / 12100) loss: 0.119956\n",
      "(Iteration 591 / 12100) loss: 0.183757\n",
      "(Iteration 601 / 12100) loss: 0.098737\n",
      "(Iteration 611 / 12100) loss: 0.160654\n",
      "(Iteration 621 / 12100) loss: 0.114858\n",
      "(Iteration 631 / 12100) loss: 0.106832\n",
      "(Iteration 641 / 12100) loss: 0.131995\n",
      "(Iteration 651 / 12100) loss: 0.088132\n",
      "(Iteration 661 / 12100) loss: 0.246481\n",
      "(Iteration 671 / 12100) loss: 0.100034\n",
      "(Iteration 681 / 12100) loss: 0.116272\n",
      "(Iteration 691 / 12100) loss: 0.099756\n",
      "(Iteration 701 / 12100) loss: 0.098224\n",
      "(Iteration 711 / 12100) loss: 0.158374\n",
      "(Iteration 721 / 12100) loss: 0.107461\n",
      "(Iteration 731 / 12100) loss: 0.175954\n",
      "(Iteration 741 / 12100) loss: 0.107474\n",
      "(Iteration 751 / 12100) loss: 0.128782\n",
      "(Iteration 761 / 12100) loss: 0.117170\n",
      "(Iteration 771 / 12100) loss: 0.155079\n",
      "(Iteration 781 / 12100) loss: 0.185360\n",
      "(Iteration 791 / 12100) loss: 0.109831\n",
      "(Iteration 801 / 12100) loss: 0.093145\n",
      "(Iteration 811 / 12100) loss: 0.155488\n",
      "(Iteration 821 / 12100) loss: 0.138743\n",
      "(Iteration 831 / 12100) loss: 0.170736\n",
      "(Iteration 841 / 12100) loss: 0.111531\n",
      "(Iteration 851 / 12100) loss: 0.112591\n",
      "(Iteration 861 / 12100) loss: 0.111922\n",
      "(Iteration 871 / 12100) loss: 0.057598\n",
      "(Iteration 881 / 12100) loss: 0.163326\n",
      "(Iteration 891 / 12100) loss: 0.121281\n",
      "(Iteration 901 / 12100) loss: 0.116019\n",
      "(Iteration 911 / 12100) loss: 0.225414\n",
      "(Iteration 921 / 12100) loss: 0.157517\n",
      "(Iteration 931 / 12100) loss: 0.092380\n",
      "(Iteration 941 / 12100) loss: 0.141797\n",
      "(Iteration 951 / 12100) loss: 0.060914\n",
      "(Iteration 961 / 12100) loss: 0.244793\n",
      "(Iteration 971 / 12100) loss: 0.129096\n",
      "(Iteration 981 / 12100) loss: 0.124183\n",
      "(Iteration 991 / 12100) loss: 0.230415\n",
      "(Iteration 1001 / 12100) loss: 0.101316\n",
      "(Epoch 2 / 5000) train acc: 0.892562; val_acc: 0.770492\n",
      "F1: 0.8055555555555556 \t Precision 0.7073170731707317 \t recall 0.9354838709677419\n",
      "(Iteration 1011 / 12100) loss: 0.186281\n",
      "(Iteration 1021 / 12100) loss: 0.086603\n",
      "(Iteration 1031 / 12100) loss: 0.070936\n",
      "(Iteration 1041 / 12100) loss: 0.142775\n",
      "(Iteration 1051 / 12100) loss: 0.068169\n",
      "(Iteration 1061 / 12100) loss: 0.134151\n",
      "(Iteration 1071 / 12100) loss: 0.107221\n",
      "(Iteration 1081 / 12100) loss: 0.114657\n",
      "(Iteration 1091 / 12100) loss: 0.057442\n",
      "(Iteration 1101 / 12100) loss: 0.067181\n",
      "(Iteration 1111 / 12100) loss: 0.197078\n",
      "(Iteration 1121 / 12100) loss: 0.036078\n",
      "(Iteration 1131 / 12100) loss: 0.043532\n",
      "(Iteration 1141 / 12100) loss: 0.091225\n",
      "(Iteration 1151 / 12100) loss: 0.060742\n",
      "(Iteration 1161 / 12100) loss: 0.182339\n",
      "(Iteration 1171 / 12100) loss: 0.120446\n",
      "(Iteration 1181 / 12100) loss: 0.076482\n",
      "(Iteration 1191 / 12100) loss: 0.117056\n",
      "(Iteration 1201 / 12100) loss: 0.113271\n",
      "(Iteration 1211 / 12100) loss: 0.066965\n",
      "(Iteration 1221 / 12100) loss: 0.059796\n",
      "(Iteration 1231 / 12100) loss: 0.191219\n",
      "(Iteration 1241 / 12100) loss: 0.161467\n",
      "(Iteration 1251 / 12100) loss: 0.076843\n",
      "(Iteration 1261 / 12100) loss: 0.116711\n",
      "(Iteration 1271 / 12100) loss: 0.106242\n",
      "(Iteration 1281 / 12100) loss: 0.129838\n",
      "(Iteration 1291 / 12100) loss: 0.038160\n",
      "(Iteration 1301 / 12100) loss: 0.054928\n",
      "(Iteration 1311 / 12100) loss: 0.117227\n",
      "(Iteration 1321 / 12100) loss: 0.084160\n",
      "(Iteration 1331 / 12100) loss: 0.140460\n",
      "(Iteration 1341 / 12100) loss: 0.145809\n",
      "(Iteration 1351 / 12100) loss: 0.090309\n",
      "(Iteration 1361 / 12100) loss: 0.079297\n",
      "(Iteration 1371 / 12100) loss: 0.121556\n",
      "(Iteration 1381 / 12100) loss: 0.155869\n",
      "(Iteration 1391 / 12100) loss: 0.124350\n",
      "(Iteration 1401 / 12100) loss: 0.062674\n",
      "(Iteration 1411 / 12100) loss: 0.075866\n",
      "(Iteration 1421 / 12100) loss: 0.139561\n",
      "(Iteration 1431 / 12100) loss: 0.091484\n",
      "(Iteration 1441 / 12100) loss: 0.095010\n",
      "(Iteration 1451 / 12100) loss: 0.081372\n",
      "(Iteration 1461 / 12100) loss: 0.105776\n",
      "(Iteration 1471 / 12100) loss: 0.137552\n",
      "(Iteration 1481 / 12100) loss: 0.105713\n",
      "(Iteration 1491 / 12100) loss: 0.059509\n",
      "(Iteration 1501 / 12100) loss: 0.090029\n",
      "(Epoch 2 / 5000) train acc: 0.909091; val_acc: 0.786885\n",
      "F1: 0.8059701492537312 \t Precision 0.75 \t recall 0.8709677419354839\n",
      "(Iteration 1511 / 12100) loss: 0.074341\n",
      "(Iteration 1521 / 12100) loss: 0.084775\n",
      "(Iteration 1531 / 12100) loss: 0.071946\n",
      "(Iteration 1541 / 12100) loss: 0.055171\n",
      "(Iteration 1551 / 12100) loss: 0.101589\n",
      "(Iteration 1561 / 12100) loss: 0.054260\n",
      "(Iteration 1571 / 12100) loss: 0.132345\n",
      "(Iteration 1581 / 12100) loss: 0.111195\n",
      "(Iteration 1591 / 12100) loss: 0.260215\n",
      "(Iteration 1601 / 12100) loss: 0.067616\n",
      "(Iteration 1611 / 12100) loss: 0.081441\n",
      "(Iteration 1621 / 12100) loss: 0.176277\n",
      "(Iteration 1631 / 12100) loss: 0.167578\n",
      "(Iteration 1641 / 12100) loss: 0.134885\n",
      "(Iteration 1651 / 12100) loss: 0.107271\n",
      "(Iteration 1661 / 12100) loss: 0.175020\n",
      "(Iteration 1671 / 12100) loss: 0.084091\n",
      "(Iteration 1681 / 12100) loss: 0.095602\n",
      "(Iteration 1691 / 12100) loss: 0.105084\n",
      "(Iteration 1701 / 12100) loss: 0.090214\n",
      "(Iteration 1711 / 12100) loss: 0.089871\n",
      "(Iteration 1721 / 12100) loss: 0.053119\n",
      "(Iteration 1731 / 12100) loss: 0.110677\n",
      "(Iteration 1741 / 12100) loss: 0.086164\n",
      "(Iteration 1751 / 12100) loss: 0.116122\n",
      "(Iteration 1761 / 12100) loss: 0.168247\n",
      "(Iteration 1771 / 12100) loss: 0.087021\n",
      "(Iteration 1781 / 12100) loss: 0.083357\n",
      "(Iteration 1791 / 12100) loss: 0.088690\n",
      "(Iteration 1801 / 12100) loss: 0.109238\n",
      "(Iteration 1811 / 12100) loss: 0.044213\n",
      "(Iteration 1821 / 12100) loss: 0.091542\n",
      "(Iteration 1831 / 12100) loss: 0.130363\n",
      "(Iteration 1841 / 12100) loss: 0.083391\n",
      "(Iteration 1851 / 12100) loss: 0.118899\n",
      "(Iteration 1861 / 12100) loss: 0.048711\n",
      "(Iteration 1871 / 12100) loss: 0.065255\n",
      "(Iteration 1881 / 12100) loss: 0.076416\n",
      "(Iteration 1891 / 12100) loss: 0.041013\n",
      "(Iteration 1901 / 12100) loss: 0.048875\n",
      "(Iteration 1911 / 12100) loss: 0.082987\n",
      "(Iteration 1921 / 12100) loss: 0.066933\n",
      "(Iteration 1931 / 12100) loss: 0.088607\n",
      "(Iteration 1941 / 12100) loss: 0.069424\n",
      "(Iteration 1951 / 12100) loss: 0.054921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1961 / 12100) loss: 0.106066\n",
      "(Iteration 1971 / 12100) loss: 0.117603\n",
      "(Iteration 1981 / 12100) loss: 0.065295\n",
      "(Iteration 1991 / 12100) loss: 0.254565\n",
      "(Iteration 2001 / 12100) loss: 0.070837\n",
      "(Epoch 3 / 5000) train acc: 0.880165; val_acc: 0.803279\n",
      "F1: 0.8285714285714285 \t Precision 0.7435897435897436 \t recall 0.9354838709677419\n",
      "(Iteration 2011 / 12100) loss: 0.072945\n",
      "(Iteration 2021 / 12100) loss: 0.171380\n",
      "(Iteration 2031 / 12100) loss: 0.065924\n",
      "(Iteration 2041 / 12100) loss: 0.055094\n",
      "(Iteration 2051 / 12100) loss: 0.077844\n",
      "(Iteration 2061 / 12100) loss: 0.049651\n",
      "(Iteration 2071 / 12100) loss: 0.077207\n",
      "(Iteration 2081 / 12100) loss: 0.038237\n",
      "(Iteration 2091 / 12100) loss: 0.052802\n",
      "(Iteration 2101 / 12100) loss: 0.051621\n",
      "(Iteration 2111 / 12100) loss: 0.162626\n",
      "(Iteration 2121 / 12100) loss: 0.062493\n",
      "(Iteration 2131 / 12100) loss: 0.131788\n",
      "(Iteration 2141 / 12100) loss: 0.081917\n",
      "(Iteration 2151 / 12100) loss: 0.083578\n",
      "(Iteration 2161 / 12100) loss: 0.071724\n",
      "(Iteration 2171 / 12100) loss: 0.108923\n",
      "(Iteration 2181 / 12100) loss: 0.051900\n",
      "(Iteration 2191 / 12100) loss: 0.054035\n",
      "(Iteration 2201 / 12100) loss: 0.080566\n",
      "(Iteration 2211 / 12100) loss: 0.052138\n",
      "(Iteration 2221 / 12100) loss: 0.078095\n",
      "(Iteration 2231 / 12100) loss: 0.080535\n",
      "(Iteration 2241 / 12100) loss: 0.054579\n",
      "(Iteration 2251 / 12100) loss: 0.049928\n",
      "(Iteration 2261 / 12100) loss: 0.113520\n",
      "(Iteration 2271 / 12100) loss: 0.069511\n",
      "(Iteration 2281 / 12100) loss: 0.095626\n",
      "(Iteration 2291 / 12100) loss: 0.067071\n",
      "(Iteration 2301 / 12100) loss: 0.069585\n",
      "(Iteration 2311 / 12100) loss: 0.095998\n",
      "(Iteration 2321 / 12100) loss: 0.119371\n",
      "(Iteration 2331 / 12100) loss: 0.096107\n",
      "(Iteration 2341 / 12100) loss: 0.093696\n",
      "(Iteration 2351 / 12100) loss: 0.090040\n",
      "(Iteration 2361 / 12100) loss: 0.042717\n",
      "(Iteration 2371 / 12100) loss: 0.033698\n",
      "(Iteration 2381 / 12100) loss: 0.104569\n",
      "(Iteration 2391 / 12100) loss: 0.070222\n",
      "(Iteration 2401 / 12100) loss: 0.043221\n",
      "(Iteration 2411 / 12100) loss: 0.133673\n",
      "(Iteration 2421 / 12100) loss: 0.037407\n",
      "(Iteration 2431 / 12100) loss: 0.057609\n",
      "(Iteration 2441 / 12100) loss: 0.144569\n",
      "(Iteration 2451 / 12100) loss: 0.031659\n",
      "(Iteration 2461 / 12100) loss: 0.048433\n",
      "(Iteration 2471 / 12100) loss: 0.106899\n",
      "(Iteration 2481 / 12100) loss: 0.068866\n",
      "(Iteration 2491 / 12100) loss: 0.094036\n",
      "(Iteration 2501 / 12100) loss: 0.037447\n",
      "(Epoch 3 / 5000) train acc: 0.909091; val_acc: 0.770492\n",
      "F1: 0.8 \t Precision 0.717948717948718 \t recall 0.9032258064516129\n",
      "(Iteration 2511 / 12100) loss: 0.055439\n",
      "(Iteration 2521 / 12100) loss: 0.075729\n",
      "(Iteration 2531 / 12100) loss: 0.076343\n",
      "(Iteration 2541 / 12100) loss: 0.076593\n",
      "(Iteration 2551 / 12100) loss: 0.034724\n",
      "(Iteration 2561 / 12100) loss: 0.035751\n",
      "(Iteration 2571 / 12100) loss: 0.099539\n",
      "(Iteration 2581 / 12100) loss: 0.064961\n",
      "(Iteration 2591 / 12100) loss: 0.084537\n",
      "(Iteration 2601 / 12100) loss: 0.067994\n",
      "(Iteration 2611 / 12100) loss: 0.028203\n",
      "(Iteration 2621 / 12100) loss: 0.053072\n",
      "(Iteration 2631 / 12100) loss: 0.082186\n",
      "(Iteration 2641 / 12100) loss: 0.040192\n",
      "(Iteration 2651 / 12100) loss: 0.097685\n",
      "(Iteration 2661 / 12100) loss: 0.037397\n",
      "(Iteration 2671 / 12100) loss: 0.096116\n",
      "(Iteration 2681 / 12100) loss: 0.107284\n",
      "(Iteration 2691 / 12100) loss: 0.069208\n",
      "(Iteration 2701 / 12100) loss: 0.054159\n",
      "(Iteration 2711 / 12100) loss: 0.044122\n",
      "(Iteration 2721 / 12100) loss: 0.099971\n",
      "(Iteration 2731 / 12100) loss: 0.045451\n",
      "(Iteration 2741 / 12100) loss: 0.044595\n",
      "(Iteration 2751 / 12100) loss: 0.047296\n",
      "(Iteration 2761 / 12100) loss: 0.054579\n",
      "(Iteration 2771 / 12100) loss: 0.096252\n",
      "(Iteration 2781 / 12100) loss: 0.070098\n",
      "(Iteration 2791 / 12100) loss: 0.108145\n",
      "(Iteration 2801 / 12100) loss: 0.128653\n",
      "(Iteration 2811 / 12100) loss: 0.051311\n",
      "(Iteration 2821 / 12100) loss: 0.087908\n",
      "(Iteration 2831 / 12100) loss: 0.078163\n",
      "(Iteration 2841 / 12100) loss: 0.051301\n",
      "(Iteration 2851 / 12100) loss: 0.062400\n",
      "(Iteration 2861 / 12100) loss: 0.066611\n",
      "(Iteration 2871 / 12100) loss: 0.069647\n",
      "(Iteration 2881 / 12100) loss: 0.041513\n",
      "(Iteration 2891 / 12100) loss: 0.112070\n",
      "(Iteration 2901 / 12100) loss: 0.067443\n",
      "(Iteration 2911 / 12100) loss: 0.079482\n",
      "(Iteration 2921 / 12100) loss: 0.088145\n",
      "(Iteration 2931 / 12100) loss: 0.035039\n",
      "(Iteration 2941 / 12100) loss: 0.090209\n",
      "(Iteration 2951 / 12100) loss: 0.084793\n",
      "(Iteration 2961 / 12100) loss: 0.144098\n",
      "(Iteration 2971 / 12100) loss: 0.063789\n",
      "(Iteration 2981 / 12100) loss: 0.100254\n",
      "(Iteration 2991 / 12100) loss: 0.039995\n",
      "(Iteration 3001 / 12100) loss: 0.066362\n",
      "(Epoch 4 / 5000) train acc: 0.929752; val_acc: 0.770492\n",
      "F1: 0.8 \t Precision 0.717948717948718 \t recall 0.9032258064516129\n",
      "(Iteration 3011 / 12100) loss: 0.070015\n",
      "(Iteration 3021 / 12100) loss: 0.120125\n",
      "(Iteration 3031 / 12100) loss: 0.041097\n",
      "(Iteration 3041 / 12100) loss: 0.050591\n",
      "(Iteration 3051 / 12100) loss: 0.122567\n",
      "(Iteration 3061 / 12100) loss: 0.113916\n",
      "(Iteration 3071 / 12100) loss: 0.061971\n",
      "(Iteration 3081 / 12100) loss: 0.041659\n",
      "(Iteration 3091 / 12100) loss: 0.075312\n",
      "(Iteration 3101 / 12100) loss: 0.138029\n",
      "(Iteration 3111 / 12100) loss: 0.040267\n",
      "(Iteration 3121 / 12100) loss: 0.093272\n",
      "(Iteration 3131 / 12100) loss: 0.042442\n",
      "(Iteration 3141 / 12100) loss: 0.027049\n",
      "(Iteration 3151 / 12100) loss: 0.064043\n",
      "(Iteration 3161 / 12100) loss: 0.097480\n",
      "(Iteration 3171 / 12100) loss: 0.027918\n",
      "(Iteration 3181 / 12100) loss: 0.085158\n",
      "(Iteration 3191 / 12100) loss: 0.047948\n",
      "(Iteration 3201 / 12100) loss: 0.082620\n",
      "(Iteration 3211 / 12100) loss: 0.071162\n",
      "(Iteration 3221 / 12100) loss: 0.091955\n",
      "(Iteration 3231 / 12100) loss: 0.044434\n",
      "(Iteration 3241 / 12100) loss: 0.039132\n",
      "(Iteration 3251 / 12100) loss: 0.163193\n",
      "(Iteration 3261 / 12100) loss: 0.062031\n",
      "(Iteration 3271 / 12100) loss: 0.084284\n",
      "(Iteration 3281 / 12100) loss: 0.047495\n",
      "(Iteration 3291 / 12100) loss: 0.086931\n",
      "(Iteration 3301 / 12100) loss: 0.075428\n",
      "(Iteration 3311 / 12100) loss: 0.051890\n",
      "(Iteration 3321 / 12100) loss: 0.076608\n",
      "(Iteration 3331 / 12100) loss: 0.067152\n",
      "(Iteration 3341 / 12100) loss: 0.031617\n",
      "(Iteration 3351 / 12100) loss: 0.052490\n",
      "(Iteration 3361 / 12100) loss: 0.045610\n",
      "(Iteration 3371 / 12100) loss: 0.074331\n",
      "(Iteration 3381 / 12100) loss: 0.070973\n",
      "(Iteration 3391 / 12100) loss: 0.129918\n",
      "(Iteration 3401 / 12100) loss: 0.053165\n",
      "(Iteration 3411 / 12100) loss: 0.039880\n",
      "(Iteration 3421 / 12100) loss: 0.038971\n",
      "(Iteration 3431 / 12100) loss: 0.043303\n",
      "(Iteration 3441 / 12100) loss: 0.081686\n",
      "(Iteration 3451 / 12100) loss: 0.085101\n",
      "(Iteration 3461 / 12100) loss: 0.042876\n",
      "(Iteration 3471 / 12100) loss: 0.113733\n",
      "(Iteration 3481 / 12100) loss: 0.113466\n",
      "(Iteration 3491 / 12100) loss: 0.089325\n",
      "(Iteration 3501 / 12100) loss: 0.027321\n",
      "(Epoch 4 / 5000) train acc: 0.913223; val_acc: 0.770492\n",
      "F1: 0.8055555555555556 \t Precision 0.7073170731707317 \t recall 0.9354838709677419\n",
      "(Iteration 3511 / 12100) loss: 0.044331\n",
      "(Iteration 3521 / 12100) loss: 0.098412\n",
      "(Iteration 3531 / 12100) loss: 0.091329\n",
      "(Iteration 3541 / 12100) loss: 0.031745\n",
      "(Iteration 3551 / 12100) loss: 0.093652\n",
      "(Iteration 3561 / 12100) loss: 0.050723\n",
      "(Iteration 3571 / 12100) loss: 0.040500\n",
      "(Iteration 3581 / 12100) loss: 0.065086\n",
      "(Iteration 3591 / 12100) loss: 0.127577\n",
      "(Iteration 3601 / 12100) loss: 0.047276\n",
      "(Iteration 3611 / 12100) loss: 0.077095\n",
      "(Iteration 3621 / 12100) loss: 0.041917\n",
      "(Iteration 3631 / 12100) loss: 0.069701\n",
      "(Iteration 3641 / 12100) loss: 0.059589\n",
      "(Iteration 3651 / 12100) loss: 0.083538\n",
      "(Iteration 3661 / 12100) loss: 0.080977\n",
      "(Iteration 3671 / 12100) loss: 0.081199\n",
      "(Iteration 3681 / 12100) loss: 0.031146\n",
      "(Iteration 3691 / 12100) loss: 0.057065\n",
      "(Iteration 3701 / 12100) loss: 0.051748\n",
      "(Iteration 3711 / 12100) loss: 0.063549\n",
      "(Iteration 3721 / 12100) loss: 0.058239\n",
      "(Iteration 3731 / 12100) loss: 0.065628\n",
      "(Iteration 3741 / 12100) loss: 0.096171\n",
      "(Iteration 3751 / 12100) loss: 0.043265\n",
      "(Iteration 3761 / 12100) loss: 0.082324\n",
      "(Iteration 3771 / 12100) loss: 0.070004\n",
      "(Iteration 3781 / 12100) loss: 0.093005\n",
      "(Iteration 3791 / 12100) loss: 0.100372\n",
      "(Iteration 3801 / 12100) loss: 0.057303\n",
      "(Iteration 3811 / 12100) loss: 0.037268\n",
      "(Iteration 3821 / 12100) loss: 0.099908\n",
      "(Iteration 3831 / 12100) loss: 0.078992\n",
      "(Iteration 3841 / 12100) loss: 0.048357\n",
      "(Iteration 3851 / 12100) loss: 0.046744\n",
      "(Iteration 3861 / 12100) loss: 0.057338\n",
      "(Iteration 3871 / 12100) loss: 0.038145\n",
      "(Iteration 3881 / 12100) loss: 0.120319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 3891 / 12100) loss: 0.150642\n",
      "(Iteration 3901 / 12100) loss: 0.040690\n",
      "(Iteration 3911 / 12100) loss: 0.109931\n",
      "(Iteration 3921 / 12100) loss: 0.115423\n",
      "(Iteration 3931 / 12100) loss: 0.050186\n",
      "(Iteration 3941 / 12100) loss: 0.134433\n",
      "(Iteration 3951 / 12100) loss: 0.059463\n",
      "(Iteration 3961 / 12100) loss: 0.069195\n",
      "(Iteration 3971 / 12100) loss: 0.063255\n",
      "(Iteration 3981 / 12100) loss: 0.140083\n",
      "(Iteration 3991 / 12100) loss: 0.100786\n",
      "(Iteration 4001 / 12100) loss: 0.051753\n",
      "(Epoch 5 / 5000) train acc: 0.917355; val_acc: 0.786885\n",
      "F1: 0.8169014084507041 \t Precision 0.725 \t recall 0.9354838709677419\n",
      "(Iteration 4011 / 12100) loss: 0.175055\n",
      "(Iteration 4021 / 12100) loss: 0.087030\n",
      "(Iteration 4031 / 12100) loss: 0.078525\n",
      "(Iteration 4041 / 12100) loss: 0.130489\n",
      "(Iteration 4051 / 12100) loss: 0.107754\n",
      "(Iteration 4061 / 12100) loss: 0.048031\n",
      "(Iteration 4071 / 12100) loss: 0.063199\n",
      "(Iteration 4081 / 12100) loss: 0.044250\n",
      "(Iteration 4091 / 12100) loss: 0.051995\n",
      "(Iteration 4101 / 12100) loss: 0.048651\n",
      "(Iteration 4111 / 12100) loss: 0.075189\n",
      "(Iteration 4121 / 12100) loss: 0.034098\n",
      "(Iteration 4131 / 12100) loss: 0.094541\n",
      "(Iteration 4141 / 12100) loss: 0.031528\n",
      "(Iteration 4151 / 12100) loss: 0.071555\n",
      "(Iteration 4161 / 12100) loss: 0.134264\n",
      "(Iteration 4171 / 12100) loss: 0.084054\n",
      "(Iteration 4181 / 12100) loss: 0.059463\n",
      "(Iteration 4191 / 12100) loss: 0.203423\n",
      "(Iteration 4201 / 12100) loss: 0.061134\n",
      "(Iteration 4211 / 12100) loss: 0.118413\n",
      "(Iteration 4221 / 12100) loss: 0.095501\n",
      "(Iteration 4231 / 12100) loss: 0.040060\n",
      "(Iteration 4241 / 12100) loss: 0.051073\n",
      "(Iteration 4251 / 12100) loss: 0.035071\n",
      "(Iteration 4261 / 12100) loss: 0.035150\n",
      "(Iteration 4271 / 12100) loss: 0.058324\n",
      "(Iteration 4281 / 12100) loss: 0.051625\n",
      "(Iteration 4291 / 12100) loss: 0.051679\n",
      "(Iteration 4301 / 12100) loss: 0.039354\n",
      "(Iteration 4311 / 12100) loss: 0.039342\n",
      "(Iteration 4321 / 12100) loss: 0.029313\n",
      "(Iteration 4331 / 12100) loss: 0.061669\n",
      "(Iteration 4341 / 12100) loss: 0.110502\n",
      "(Iteration 4351 / 12100) loss: 0.036247\n",
      "(Iteration 4361 / 12100) loss: 0.117839\n",
      "(Iteration 4371 / 12100) loss: 0.072891\n",
      "(Iteration 4381 / 12100) loss: 0.035133\n",
      "(Iteration 4391 / 12100) loss: 0.035642\n",
      "(Iteration 4401 / 12100) loss: 0.071934\n",
      "(Iteration 4411 / 12100) loss: 0.040026\n",
      "(Iteration 4421 / 12100) loss: 0.047929\n",
      "(Iteration 4431 / 12100) loss: 0.042065\n",
      "(Iteration 4441 / 12100) loss: 0.063810\n",
      "(Iteration 4451 / 12100) loss: 0.039789\n",
      "(Iteration 4461 / 12100) loss: 0.035054\n",
      "(Iteration 4471 / 12100) loss: 0.039529\n",
      "(Iteration 4481 / 12100) loss: 0.090019\n",
      "(Iteration 4491 / 12100) loss: 0.040451\n",
      "(Iteration 4501 / 12100) loss: 0.071003\n",
      "(Epoch 5 / 5000) train acc: 0.904959; val_acc: 0.770492\n",
      "F1: 0.8055555555555556 \t Precision 0.7073170731707317 \t recall 0.9354838709677419\n",
      "(Iteration 4511 / 12100) loss: 0.064581\n",
      "(Iteration 4521 / 12100) loss: 0.044552\n",
      "(Iteration 4531 / 12100) loss: 0.051419\n",
      "(Iteration 4541 / 12100) loss: 0.050193\n",
      "(Iteration 4551 / 12100) loss: 0.038748\n",
      "(Iteration 4561 / 12100) loss: 0.067586\n",
      "(Iteration 4571 / 12100) loss: 0.065386\n",
      "(Iteration 4581 / 12100) loss: 0.046325\n",
      "(Iteration 4591 / 12100) loss: 0.059840\n",
      "(Iteration 4601 / 12100) loss: 0.055560\n",
      "(Iteration 4611 / 12100) loss: 0.068531\n",
      "(Iteration 4621 / 12100) loss: 0.033263\n",
      "(Iteration 4631 / 12100) loss: 0.042663\n",
      "(Iteration 4641 / 12100) loss: 0.069288\n",
      "(Iteration 4651 / 12100) loss: 0.067724\n",
      "(Iteration 4661 / 12100) loss: 0.061119\n",
      "(Iteration 4671 / 12100) loss: 0.048968\n",
      "(Iteration 4681 / 12100) loss: 0.035004\n",
      "(Iteration 4691 / 12100) loss: 0.026885\n",
      "(Iteration 4701 / 12100) loss: 0.044095\n",
      "(Iteration 4711 / 12100) loss: 0.029494\n",
      "(Iteration 4721 / 12100) loss: 0.038194\n",
      "(Iteration 4731 / 12100) loss: 0.097099\n",
      "(Iteration 4741 / 12100) loss: 0.084979\n",
      "(Iteration 4751 / 12100) loss: 0.030731\n",
      "(Iteration 4761 / 12100) loss: 0.060690\n",
      "(Iteration 4771 / 12100) loss: 0.039772\n",
      "(Iteration 4781 / 12100) loss: 0.081842\n",
      "(Iteration 4791 / 12100) loss: 0.033169\n",
      "(Iteration 4801 / 12100) loss: 0.052575\n",
      "(Iteration 4811 / 12100) loss: 0.064275\n",
      "(Iteration 4821 / 12100) loss: 0.063149\n",
      "(Iteration 4831 / 12100) loss: 0.070064\n",
      "(Iteration 4841 / 12100) loss: 0.045013\n",
      "(Iteration 4851 / 12100) loss: 0.040368\n",
      "(Iteration 4861 / 12100) loss: 0.039914\n",
      "(Iteration 4871 / 12100) loss: 0.042861\n",
      "(Iteration 4881 / 12100) loss: 0.069417\n",
      "(Iteration 4891 / 12100) loss: 0.115984\n",
      "(Iteration 4901 / 12100) loss: 0.176726\n",
      "(Iteration 4911 / 12100) loss: 0.072425\n",
      "(Iteration 4921 / 12100) loss: 0.061003\n",
      "(Iteration 4931 / 12100) loss: 0.120592\n",
      "(Iteration 4941 / 12100) loss: 0.067217\n",
      "(Iteration 4951 / 12100) loss: 0.050861\n",
      "(Iteration 4961 / 12100) loss: 0.033548\n",
      "(Iteration 4971 / 12100) loss: 0.042279\n",
      "(Iteration 4981 / 12100) loss: 0.187910\n",
      "(Iteration 4991 / 12100) loss: 0.031908\n",
      "(Iteration 5001 / 12100) loss: 0.041139\n",
      "(Epoch 6 / 5000) train acc: 0.896694; val_acc: 0.770492\n",
      "F1: 0.8 \t Precision 0.717948717948718 \t recall 0.9032258064516129\n",
      "(Iteration 5011 / 12100) loss: 0.051656\n",
      "(Iteration 5021 / 12100) loss: 0.059237\n",
      "(Iteration 5031 / 12100) loss: 0.030441\n",
      "(Iteration 5041 / 12100) loss: 0.039803\n",
      "(Iteration 5051 / 12100) loss: 0.101363\n",
      "(Iteration 5061 / 12100) loss: 0.027456\n",
      "(Iteration 5071 / 12100) loss: 0.065875\n",
      "(Iteration 5081 / 12100) loss: 0.077541\n",
      "(Iteration 5091 / 12100) loss: 0.047495\n",
      "(Iteration 5101 / 12100) loss: 0.029811\n",
      "(Iteration 5111 / 12100) loss: 0.037257\n",
      "(Iteration 5121 / 12100) loss: 0.086760\n",
      "(Iteration 5131 / 12100) loss: 0.044399\n",
      "(Iteration 5141 / 12100) loss: 0.113734\n",
      "(Iteration 5151 / 12100) loss: 0.039544\n",
      "(Iteration 5161 / 12100) loss: 0.053903\n",
      "(Iteration 5171 / 12100) loss: 0.073316\n",
      "(Iteration 5181 / 12100) loss: 0.072551\n",
      "(Iteration 5191 / 12100) loss: 0.051532\n",
      "(Iteration 5201 / 12100) loss: 0.053526\n",
      "(Iteration 5211 / 12100) loss: 0.042371\n",
      "(Iteration 5221 / 12100) loss: 0.107575\n",
      "(Iteration 5231 / 12100) loss: 0.151737\n",
      "(Iteration 5241 / 12100) loss: 0.090690\n",
      "(Iteration 5251 / 12100) loss: 0.039962\n",
      "(Iteration 5261 / 12100) loss: 0.039455\n",
      "(Iteration 5271 / 12100) loss: 0.057387\n",
      "(Iteration 5281 / 12100) loss: 0.064488\n",
      "(Iteration 5291 / 12100) loss: 0.050220\n",
      "(Iteration 5301 / 12100) loss: 0.052152\n",
      "(Iteration 5311 / 12100) loss: 0.108370\n",
      "(Iteration 5321 / 12100) loss: 0.141069\n",
      "(Iteration 5331 / 12100) loss: 0.052428\n",
      "(Iteration 5341 / 12100) loss: 0.049583\n",
      "(Iteration 5351 / 12100) loss: 0.029989\n",
      "(Iteration 5361 / 12100) loss: 0.117707\n",
      "(Iteration 5371 / 12100) loss: 0.034879\n",
      "(Iteration 5381 / 12100) loss: 0.029564\n",
      "(Iteration 5391 / 12100) loss: 0.030296\n",
      "(Iteration 5401 / 12100) loss: 0.039105\n",
      "(Iteration 5411 / 12100) loss: 0.058476\n",
      "(Iteration 5421 / 12100) loss: 0.043413\n",
      "(Iteration 5431 / 12100) loss: 0.072725\n",
      "(Iteration 5441 / 12100) loss: 0.091825\n",
      "(Iteration 5451 / 12100) loss: 0.074831\n",
      "(Iteration 5461 / 12100) loss: 0.064174\n",
      "(Iteration 5471 / 12100) loss: 0.074248\n",
      "(Iteration 5481 / 12100) loss: 0.040610\n",
      "(Iteration 5491 / 12100) loss: 0.048452\n",
      "(Iteration 5501 / 12100) loss: 0.047070\n",
      "(Epoch 6 / 5000) train acc: 0.896694; val_acc: 0.754098\n",
      "F1: 0.7887323943661972 \t Precision 0.7 \t recall 0.9032258064516129\n",
      "(Iteration 5511 / 12100) loss: 0.047857\n",
      "(Iteration 5521 / 12100) loss: 0.117158\n",
      "(Iteration 5531 / 12100) loss: 0.090926\n",
      "(Iteration 5541 / 12100) loss: 0.039184\n",
      "(Iteration 5551 / 12100) loss: 0.027313\n",
      "(Iteration 5561 / 12100) loss: 0.027904\n",
      "(Iteration 5571 / 12100) loss: 0.083467\n",
      "(Iteration 5581 / 12100) loss: 0.028857\n",
      "(Iteration 5591 / 12100) loss: 0.067530\n",
      "(Iteration 5601 / 12100) loss: 0.030641\n",
      "(Iteration 5611 / 12100) loss: 0.038778\n",
      "(Iteration 5621 / 12100) loss: 0.044482\n",
      "(Iteration 5631 / 12100) loss: 0.045269\n",
      "(Iteration 5641 / 12100) loss: 0.035866\n",
      "(Iteration 5651 / 12100) loss: 0.029836\n",
      "(Iteration 5661 / 12100) loss: 0.101802\n",
      "(Iteration 5671 / 12100) loss: 0.027537\n",
      "(Iteration 5681 / 12100) loss: 0.031227\n",
      "(Iteration 5691 / 12100) loss: 0.053431\n",
      "(Iteration 5701 / 12100) loss: 0.031728\n",
      "(Iteration 5711 / 12100) loss: 0.050474\n",
      "(Iteration 5721 / 12100) loss: 0.052171\n",
      "(Iteration 5731 / 12100) loss: 0.039064\n",
      "(Iteration 5741 / 12100) loss: 0.030596\n",
      "(Iteration 5751 / 12100) loss: 0.030752\n",
      "(Iteration 5761 / 12100) loss: 0.033368\n",
      "(Iteration 5771 / 12100) loss: 0.032993\n",
      "(Iteration 5781 / 12100) loss: 0.047631\n",
      "(Iteration 5791 / 12100) loss: 0.034702\n",
      "(Iteration 5801 / 12100) loss: 0.126355\n",
      "(Iteration 5811 / 12100) loss: 0.047291\n",
      "(Iteration 5821 / 12100) loss: 0.030831\n",
      "(Iteration 5831 / 12100) loss: 0.064208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 5841 / 12100) loss: 0.049672\n",
      "(Iteration 5851 / 12100) loss: 0.059894\n",
      "(Iteration 5861 / 12100) loss: 0.079769\n",
      "(Iteration 5871 / 12100) loss: 0.070607\n",
      "(Iteration 5881 / 12100) loss: 0.045611\n",
      "(Iteration 5891 / 12100) loss: 0.047424\n",
      "(Iteration 5901 / 12100) loss: 0.027778\n",
      "(Iteration 5911 / 12100) loss: 0.045660\n",
      "(Iteration 5921 / 12100) loss: 0.082671\n",
      "(Iteration 5931 / 12100) loss: 0.046416\n",
      "(Iteration 5941 / 12100) loss: 0.242158\n",
      "(Iteration 5951 / 12100) loss: 0.039884\n",
      "(Iteration 5961 / 12100) loss: 0.038172\n",
      "(Iteration 5971 / 12100) loss: 0.079109\n",
      "(Iteration 5981 / 12100) loss: 0.033887\n",
      "(Iteration 5991 / 12100) loss: 0.040731\n",
      "(Iteration 6001 / 12100) loss: 0.030723\n",
      "(Epoch 7 / 5000) train acc: 0.871901; val_acc: 0.770492\n",
      "F1: 0.8055555555555556 \t Precision 0.7073170731707317 \t recall 0.9354838709677419\n",
      "(Iteration 6011 / 12100) loss: 0.029125\n",
      "(Iteration 6021 / 12100) loss: 0.050239\n",
      "(Iteration 6031 / 12100) loss: 0.047852\n",
      "(Iteration 6041 / 12100) loss: 0.035077\n",
      "(Iteration 6051 / 12100) loss: 0.046940\n",
      "(Iteration 6061 / 12100) loss: 0.128008\n",
      "(Iteration 6071 / 12100) loss: 0.091099\n",
      "(Iteration 6081 / 12100) loss: 0.113405\n",
      "(Iteration 6091 / 12100) loss: 0.051117\n",
      "(Iteration 6101 / 12100) loss: 0.053294\n",
      "(Iteration 6111 / 12100) loss: 0.056688\n",
      "(Iteration 6121 / 12100) loss: 0.123439\n",
      "(Iteration 6131 / 12100) loss: 0.268478\n",
      "(Iteration 6141 / 12100) loss: 0.053532\n",
      "(Iteration 6151 / 12100) loss: 0.045245\n",
      "(Iteration 6161 / 12100) loss: 0.044636\n",
      "(Iteration 6171 / 12100) loss: 0.036234\n",
      "(Iteration 6181 / 12100) loss: 0.034712\n",
      "(Iteration 6191 / 12100) loss: 0.054889\n",
      "(Iteration 6201 / 12100) loss: 0.036029\n",
      "(Iteration 6211 / 12100) loss: 0.063790\n",
      "(Iteration 6221 / 12100) loss: 0.071093\n",
      "(Iteration 6231 / 12100) loss: 0.066628\n",
      "(Iteration 6241 / 12100) loss: 0.111257\n",
      "(Iteration 6251 / 12100) loss: 0.050206\n",
      "(Iteration 6261 / 12100) loss: 0.099505\n",
      "(Iteration 6271 / 12100) loss: 0.072812\n",
      "(Iteration 6281 / 12100) loss: 0.029513\n",
      "(Iteration 6291 / 12100) loss: 0.056391\n",
      "(Iteration 6301 / 12100) loss: 0.107604\n",
      "(Iteration 6311 / 12100) loss: 0.061306\n",
      "(Iteration 6321 / 12100) loss: 0.059853\n",
      "(Iteration 6331 / 12100) loss: 0.063063\n",
      "(Iteration 6341 / 12100) loss: 0.033628\n",
      "(Iteration 6351 / 12100) loss: 0.046163\n",
      "(Iteration 6361 / 12100) loss: 0.077065\n",
      "(Iteration 6371 / 12100) loss: 0.046012\n",
      "(Iteration 6381 / 12100) loss: 0.044890\n",
      "(Iteration 6391 / 12100) loss: 0.131175\n",
      "(Iteration 6401 / 12100) loss: 0.031174\n",
      "(Iteration 6411 / 12100) loss: 0.094314\n",
      "(Iteration 6421 / 12100) loss: 0.066828\n",
      "(Iteration 6431 / 12100) loss: 0.151831\n",
      "(Iteration 6441 / 12100) loss: 0.059515\n",
      "(Iteration 6451 / 12100) loss: 0.165462\n",
      "(Iteration 6461 / 12100) loss: 0.071341\n",
      "(Iteration 6471 / 12100) loss: 0.044353\n",
      "(Iteration 6481 / 12100) loss: 0.047019\n",
      "(Iteration 6491 / 12100) loss: 0.050503\n",
      "(Iteration 6501 / 12100) loss: 0.048503\n",
      "(Epoch 7 / 5000) train acc: 0.896694; val_acc: 0.803279\n",
      "F1: 0.8181818181818182 \t Precision 0.7714285714285715 \t recall 0.8709677419354839\n",
      "(Iteration 6511 / 12100) loss: 0.036502\n",
      "(Iteration 6521 / 12100) loss: 0.080613\n",
      "(Iteration 6531 / 12100) loss: 0.067545\n",
      "(Iteration 6541 / 12100) loss: 0.059592\n",
      "(Iteration 6551 / 12100) loss: 0.084020\n",
      "(Iteration 6561 / 12100) loss: 0.036629\n",
      "(Iteration 6571 / 12100) loss: 0.111813\n",
      "(Iteration 6581 / 12100) loss: 0.035180\n",
      "(Iteration 6591 / 12100) loss: 0.055047\n",
      "(Iteration 6601 / 12100) loss: 0.034002\n",
      "(Iteration 6611 / 12100) loss: 0.059105\n",
      "(Iteration 6621 / 12100) loss: 0.042870\n",
      "(Iteration 6631 / 12100) loss: 0.095761\n",
      "(Iteration 6641 / 12100) loss: 0.030821\n",
      "(Iteration 6651 / 12100) loss: 0.109619\n",
      "(Iteration 6661 / 12100) loss: 0.051073\n",
      "(Iteration 6671 / 12100) loss: 0.034157\n",
      "(Iteration 6681 / 12100) loss: 0.086911\n",
      "(Iteration 6691 / 12100) loss: 0.026378\n",
      "(Iteration 6701 / 12100) loss: 0.050499\n",
      "(Iteration 6711 / 12100) loss: 0.068384\n",
      "(Iteration 6721 / 12100) loss: 0.032527\n",
      "(Iteration 6731 / 12100) loss: 0.056558\n",
      "(Iteration 6741 / 12100) loss: 0.028869\n",
      "(Iteration 6751 / 12100) loss: 0.052979\n",
      "(Iteration 6761 / 12100) loss: 0.061039\n",
      "(Iteration 6771 / 12100) loss: 0.034417\n",
      "(Iteration 6781 / 12100) loss: 0.053978\n",
      "(Iteration 6791 / 12100) loss: 0.090565\n",
      "(Iteration 6801 / 12100) loss: 0.040666\n",
      "(Iteration 6811 / 12100) loss: 0.104515\n",
      "(Iteration 6821 / 12100) loss: 0.038361\n",
      "(Iteration 6831 / 12100) loss: 0.162067\n",
      "(Iteration 6841 / 12100) loss: 0.091370\n",
      "(Iteration 6851 / 12100) loss: 0.067132\n",
      "(Iteration 6861 / 12100) loss: 0.041962\n",
      "(Iteration 6871 / 12100) loss: 0.155451\n",
      "(Iteration 6881 / 12100) loss: 0.045699\n",
      "(Iteration 6891 / 12100) loss: 0.095703\n",
      "(Iteration 6901 / 12100) loss: 0.031859\n",
      "(Iteration 6911 / 12100) loss: 0.042456\n",
      "(Iteration 6921 / 12100) loss: 0.275778\n",
      "(Iteration 6931 / 12100) loss: 0.107919\n",
      "(Iteration 6941 / 12100) loss: 0.097715\n",
      "(Iteration 6951 / 12100) loss: 0.035919\n",
      "(Iteration 6961 / 12100) loss: 0.038595\n",
      "(Iteration 6971 / 12100) loss: 0.031575\n",
      "(Iteration 6981 / 12100) loss: 0.029324\n",
      "(Iteration 6991 / 12100) loss: 0.027692\n",
      "(Iteration 7001 / 12100) loss: 0.157226\n",
      "(Epoch 8 / 5000) train acc: 0.884298; val_acc: 0.786885\n",
      "F1: 0.821917808219178 \t Precision 0.7142857142857143 \t recall 0.967741935483871\n",
      "(Iteration 7011 / 12100) loss: 0.048341\n",
      "(Iteration 7021 / 12100) loss: 0.055947\n",
      "(Iteration 7031 / 12100) loss: 0.131490\n",
      "(Iteration 7041 / 12100) loss: 0.027913\n",
      "(Iteration 7051 / 12100) loss: 0.082092\n",
      "(Iteration 7061 / 12100) loss: 0.029740\n",
      "(Iteration 7071 / 12100) loss: 0.075316\n",
      "(Iteration 7081 / 12100) loss: 0.064521\n",
      "(Iteration 7091 / 12100) loss: 0.085237\n",
      "(Iteration 7101 / 12100) loss: 0.104681\n",
      "(Iteration 7111 / 12100) loss: 0.088567\n",
      "(Iteration 7121 / 12100) loss: 0.035157\n",
      "(Iteration 7131 / 12100) loss: 0.075967\n",
      "(Iteration 7141 / 12100) loss: 0.102879\n",
      "(Iteration 7151 / 12100) loss: 0.067982\n",
      "(Iteration 7161 / 12100) loss: 0.068015\n",
      "(Iteration 7171 / 12100) loss: 0.037453\n",
      "(Iteration 7181 / 12100) loss: 0.039968\n",
      "(Iteration 7191 / 12100) loss: 0.046894\n",
      "(Iteration 7201 / 12100) loss: 0.083881\n",
      "(Iteration 7211 / 12100) loss: 0.053059\n",
      "(Iteration 7221 / 12100) loss: 0.037743\n",
      "(Iteration 7231 / 12100) loss: 0.081733\n",
      "(Iteration 7241 / 12100) loss: 0.058455\n",
      "(Iteration 7251 / 12100) loss: 0.030044\n",
      "(Iteration 7261 / 12100) loss: 0.034320\n",
      "(Iteration 7271 / 12100) loss: 0.029050\n",
      "(Iteration 7281 / 12100) loss: 0.045452\n",
      "(Iteration 7291 / 12100) loss: 0.028675\n",
      "(Iteration 7301 / 12100) loss: 0.032452\n",
      "(Iteration 7311 / 12100) loss: 0.037518\n",
      "(Iteration 7321 / 12100) loss: 0.039032\n",
      "(Iteration 7331 / 12100) loss: 0.105342\n",
      "(Iteration 7341 / 12100) loss: 0.031852\n",
      "(Iteration 7351 / 12100) loss: 0.060144\n",
      "(Iteration 7361 / 12100) loss: 0.071914\n",
      "(Iteration 7371 / 12100) loss: 0.050615\n",
      "(Iteration 7381 / 12100) loss: 0.029410\n",
      "(Iteration 7391 / 12100) loss: 0.045151\n",
      "(Iteration 7401 / 12100) loss: 0.034352\n",
      "(Iteration 7411 / 12100) loss: 0.037790\n",
      "(Iteration 7421 / 12100) loss: 0.041727\n",
      "(Iteration 7431 / 12100) loss: 0.049766\n",
      "(Iteration 7441 / 12100) loss: 0.026483\n",
      "(Iteration 7451 / 12100) loss: 0.108982\n",
      "(Iteration 7461 / 12100) loss: 0.108981\n",
      "(Iteration 7471 / 12100) loss: 0.063659\n",
      "(Iteration 7481 / 12100) loss: 0.034468\n",
      "(Iteration 7491 / 12100) loss: 0.033490\n",
      "(Iteration 7501 / 12100) loss: 0.026961\n",
      "(Epoch 8 / 5000) train acc: 0.900826; val_acc: 0.737705\n",
      "F1: 0.7777777777777778 \t Precision 0.6829268292682927 \t recall 0.9032258064516129\n",
      "(Iteration 7511 / 12100) loss: 0.057070\n",
      "(Iteration 7521 / 12100) loss: 0.048285\n",
      "(Iteration 7531 / 12100) loss: 0.031457\n",
      "(Iteration 7541 / 12100) loss: 0.030136\n",
      "(Iteration 7551 / 12100) loss: 0.046201\n",
      "(Iteration 7561 / 12100) loss: 0.041718\n",
      "(Iteration 7571 / 12100) loss: 0.053322\n",
      "(Iteration 7581 / 12100) loss: 0.030204\n",
      "(Iteration 7591 / 12100) loss: 0.040454\n",
      "(Iteration 7601 / 12100) loss: 0.028691\n",
      "(Iteration 7611 / 12100) loss: 0.045785\n",
      "(Iteration 7621 / 12100) loss: 0.032506\n",
      "(Iteration 7631 / 12100) loss: 0.037728\n",
      "(Iteration 7641 / 12100) loss: 0.041419\n",
      "(Iteration 7651 / 12100) loss: 0.065489\n",
      "(Iteration 7661 / 12100) loss: 0.051420\n",
      "(Iteration 7671 / 12100) loss: 0.056435\n",
      "(Iteration 7681 / 12100) loss: 0.130330\n",
      "(Iteration 7691 / 12100) loss: 0.058534\n",
      "(Iteration 7701 / 12100) loss: 0.109446\n",
      "(Iteration 7711 / 12100) loss: 0.124764\n",
      "(Iteration 7721 / 12100) loss: 0.096403\n",
      "(Iteration 7731 / 12100) loss: 0.045789\n",
      "(Iteration 7741 / 12100) loss: 0.057111\n",
      "(Iteration 7751 / 12100) loss: 0.096915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 7761 / 12100) loss: 0.030538\n",
      "(Iteration 7771 / 12100) loss: 0.061020\n",
      "(Iteration 7781 / 12100) loss: 0.031015\n",
      "(Iteration 7791 / 12100) loss: 0.054497\n",
      "(Iteration 7801 / 12100) loss: 0.090712\n",
      "(Iteration 7811 / 12100) loss: 0.057784\n",
      "(Iteration 7821 / 12100) loss: 0.042186\n",
      "(Iteration 7831 / 12100) loss: 0.057009\n",
      "(Iteration 7841 / 12100) loss: 0.247812\n",
      "(Iteration 7851 / 12100) loss: 0.079150\n",
      "(Iteration 7861 / 12100) loss: 0.066606\n",
      "(Iteration 7871 / 12100) loss: 0.049402\n",
      "(Iteration 7881 / 12100) loss: 0.042781\n",
      "(Iteration 7891 / 12100) loss: 0.107798\n",
      "(Iteration 7901 / 12100) loss: 0.048804\n",
      "(Iteration 7911 / 12100) loss: 0.053632\n",
      "(Iteration 7921 / 12100) loss: 0.078413\n",
      "(Iteration 7931 / 12100) loss: 0.029209\n",
      "(Iteration 7941 / 12100) loss: 0.070701\n",
      "(Iteration 7951 / 12100) loss: 0.055668\n",
      "(Iteration 7961 / 12100) loss: 0.036973\n",
      "(Iteration 7971 / 12100) loss: 0.030927\n",
      "(Iteration 7981 / 12100) loss: 0.067290\n",
      "(Iteration 7991 / 12100) loss: 0.050836\n",
      "(Iteration 8001 / 12100) loss: 0.074091\n",
      "(Epoch 9 / 5000) train acc: 0.884298; val_acc: 0.770492\n",
      "F1: 0.810810810810811 \t Precision 0.6976744186046512 \t recall 0.967741935483871\n",
      "(Iteration 8011 / 12100) loss: 0.041920\n",
      "(Iteration 8021 / 12100) loss: 0.035408\n",
      "(Iteration 8031 / 12100) loss: 0.041466\n",
      "(Iteration 8041 / 12100) loss: 0.041595\n",
      "(Iteration 8051 / 12100) loss: 0.032799\n",
      "(Iteration 8061 / 12100) loss: 0.029324\n",
      "(Iteration 8071 / 12100) loss: 0.032686\n",
      "(Iteration 8081 / 12100) loss: 0.043975\n",
      "(Iteration 8091 / 12100) loss: 0.047019\n",
      "(Iteration 8101 / 12100) loss: 0.029242\n",
      "(Iteration 8111 / 12100) loss: 0.079527\n",
      "(Iteration 8121 / 12100) loss: 0.066693\n",
      "(Iteration 8131 / 12100) loss: 0.095014\n",
      "(Iteration 8141 / 12100) loss: 0.073339\n",
      "(Iteration 8151 / 12100) loss: 0.037662\n",
      "(Iteration 8161 / 12100) loss: 0.043281\n",
      "(Iteration 8171 / 12100) loss: 0.099259\n",
      "(Iteration 8181 / 12100) loss: 0.039105\n",
      "(Iteration 8191 / 12100) loss: 0.136481\n",
      "(Iteration 8201 / 12100) loss: 0.072049\n",
      "(Iteration 8211 / 12100) loss: 0.034291\n",
      "(Iteration 8221 / 12100) loss: 0.028990\n",
      "(Iteration 8231 / 12100) loss: 0.043552\n",
      "(Iteration 8241 / 12100) loss: 0.064160\n",
      "(Iteration 8251 / 12100) loss: 0.030386\n",
      "(Iteration 8261 / 12100) loss: 0.033205\n",
      "(Iteration 8271 / 12100) loss: 0.041106\n",
      "(Iteration 8281 / 12100) loss: 0.073631\n",
      "(Iteration 8291 / 12100) loss: 0.051789\n",
      "(Iteration 8301 / 12100) loss: 0.108475\n",
      "(Iteration 8311 / 12100) loss: 0.037656\n",
      "(Iteration 8321 / 12100) loss: 0.036939\n",
      "(Iteration 8331 / 12100) loss: 0.036439\n",
      "(Iteration 8341 / 12100) loss: 0.087515\n",
      "(Iteration 8351 / 12100) loss: 0.048051\n",
      "(Iteration 8361 / 12100) loss: 0.030711\n",
      "(Iteration 8371 / 12100) loss: 0.082639\n",
      "(Iteration 8381 / 12100) loss: 0.043380\n",
      "(Iteration 8391 / 12100) loss: 0.032092\n",
      "(Iteration 8401 / 12100) loss: 0.034968\n",
      "(Iteration 8411 / 12100) loss: 0.055050\n",
      "(Iteration 8421 / 12100) loss: 0.039344\n",
      "(Iteration 8431 / 12100) loss: 0.071906\n",
      "(Iteration 8441 / 12100) loss: 0.068714\n",
      "(Iteration 8451 / 12100) loss: 0.074238\n",
      "(Iteration 8461 / 12100) loss: 0.078223\n",
      "(Iteration 8471 / 12100) loss: 0.098972\n",
      "(Iteration 8481 / 12100) loss: 0.089736\n",
      "(Iteration 8491 / 12100) loss: 0.087761\n",
      "(Iteration 8501 / 12100) loss: 0.071594\n",
      "(Epoch 9 / 5000) train acc: 0.876033; val_acc: 0.803279\n",
      "F1: 0.8285714285714285 \t Precision 0.7435897435897436 \t recall 0.9354838709677419\n",
      "(Iteration 8511 / 12100) loss: 0.042891\n",
      "(Iteration 8521 / 12100) loss: 0.077057\n",
      "(Iteration 8531 / 12100) loss: 0.040831\n",
      "(Iteration 8541 / 12100) loss: 0.078264\n",
      "(Iteration 8551 / 12100) loss: 0.059785\n",
      "(Iteration 8561 / 12100) loss: 0.032054\n",
      "(Iteration 8571 / 12100) loss: 0.074807\n",
      "(Iteration 8581 / 12100) loss: 0.066188\n",
      "(Iteration 8591 / 12100) loss: 0.038403\n",
      "(Iteration 8601 / 12100) loss: 0.039430\n",
      "(Iteration 8611 / 12100) loss: 0.080639\n",
      "(Iteration 8621 / 12100) loss: 0.076766\n",
      "(Iteration 8631 / 12100) loss: 0.071325\n",
      "(Iteration 8641 / 12100) loss: 0.041715\n",
      "(Iteration 8651 / 12100) loss: 0.051121\n",
      "(Iteration 8661 / 12100) loss: 0.027335\n",
      "(Iteration 8671 / 12100) loss: 0.031527\n",
      "(Iteration 8681 / 12100) loss: 0.069784\n",
      "(Iteration 8691 / 12100) loss: 0.062669\n",
      "(Iteration 8701 / 12100) loss: 0.037462\n",
      "(Iteration 8711 / 12100) loss: 0.037329\n",
      "(Iteration 8721 / 12100) loss: 0.063031\n",
      "(Iteration 8731 / 12100) loss: 0.159416\n",
      "(Iteration 8741 / 12100) loss: 0.067090\n",
      "(Iteration 8751 / 12100) loss: 0.047633\n",
      "(Iteration 8761 / 12100) loss: 0.037314\n",
      "(Iteration 8771 / 12100) loss: 0.054934\n",
      "(Iteration 8781 / 12100) loss: 0.082335\n",
      "(Iteration 8791 / 12100) loss: 0.119204\n",
      "(Iteration 8801 / 12100) loss: 0.094166\n",
      "(Iteration 8811 / 12100) loss: 0.059440\n",
      "(Iteration 8821 / 12100) loss: 0.043090\n",
      "(Iteration 8831 / 12100) loss: 0.092739\n",
      "(Iteration 8841 / 12100) loss: 0.053059\n",
      "(Iteration 8851 / 12100) loss: 0.048213\n",
      "(Iteration 8861 / 12100) loss: 0.063972\n",
      "(Iteration 8871 / 12100) loss: 0.036105\n",
      "(Iteration 8881 / 12100) loss: 0.061577\n",
      "(Iteration 8891 / 12100) loss: 0.122223\n",
      "(Iteration 8901 / 12100) loss: 0.092759\n",
      "(Iteration 8911 / 12100) loss: 0.054074\n",
      "(Iteration 8921 / 12100) loss: 0.062143\n",
      "(Iteration 8931 / 12100) loss: 0.034087\n",
      "(Iteration 8941 / 12100) loss: 0.045990\n",
      "(Iteration 8951 / 12100) loss: 0.052820\n",
      "(Iteration 8961 / 12100) loss: 0.037362\n",
      "(Iteration 8971 / 12100) loss: 0.044720\n",
      "(Iteration 8981 / 12100) loss: 0.054764\n",
      "(Iteration 8991 / 12100) loss: 0.118433\n",
      "(Iteration 9001 / 12100) loss: 0.032197\n",
      "(Epoch 10 / 5000) train acc: 0.880165; val_acc: 0.770492\n",
      "F1: 0.810810810810811 \t Precision 0.6976744186046512 \t recall 0.967741935483871\n",
      "(Iteration 9011 / 12100) loss: 0.038592\n",
      "(Iteration 9021 / 12100) loss: 0.048531\n",
      "(Iteration 9031 / 12100) loss: 0.043638\n",
      "(Iteration 9041 / 12100) loss: 0.085540\n",
      "(Iteration 9051 / 12100) loss: 0.035299\n",
      "(Iteration 9061 / 12100) loss: 0.032767\n",
      "(Iteration 9071 / 12100) loss: 0.054729\n",
      "(Iteration 9081 / 12100) loss: 0.039482\n",
      "(Iteration 9091 / 12100) loss: 0.053841\n",
      "(Iteration 9101 / 12100) loss: 0.091011\n",
      "(Iteration 9111 / 12100) loss: 0.037851\n",
      "(Iteration 9121 / 12100) loss: 0.150077\n",
      "(Iteration 9131 / 12100) loss: 0.045696\n",
      "(Iteration 9141 / 12100) loss: 0.065924\n",
      "(Iteration 9151 / 12100) loss: 0.079018\n",
      "(Iteration 9161 / 12100) loss: 0.049130\n",
      "(Iteration 9171 / 12100) loss: 0.067931\n",
      "(Iteration 9181 / 12100) loss: 0.031622\n",
      "(Iteration 9191 / 12100) loss: 0.065966\n",
      "(Iteration 9201 / 12100) loss: 0.038762\n",
      "(Iteration 9211 / 12100) loss: 0.037979\n",
      "(Iteration 9221 / 12100) loss: 0.028961\n",
      "(Iteration 9231 / 12100) loss: 0.046876\n",
      "(Iteration 9241 / 12100) loss: 0.065614\n",
      "(Iteration 9251 / 12100) loss: 0.029219\n",
      "(Iteration 9261 / 12100) loss: 0.036971\n",
      "(Iteration 9271 / 12100) loss: 0.042110\n",
      "(Iteration 9281 / 12100) loss: 0.067557\n",
      "(Iteration 9291 / 12100) loss: 0.042754\n",
      "(Iteration 9301 / 12100) loss: 0.146971\n",
      "(Iteration 9311 / 12100) loss: 0.031728\n",
      "(Iteration 9321 / 12100) loss: 0.043338\n",
      "(Iteration 9331 / 12100) loss: 0.041132\n",
      "(Iteration 9341 / 12100) loss: 0.068668\n",
      "(Iteration 9351 / 12100) loss: 0.051417\n",
      "(Iteration 9361 / 12100) loss: 0.057055\n",
      "(Iteration 9371 / 12100) loss: 0.034085\n",
      "(Iteration 9381 / 12100) loss: 0.070003\n",
      "(Iteration 9391 / 12100) loss: 0.036899\n",
      "(Iteration 9401 / 12100) loss: 0.095888\n",
      "(Iteration 9411 / 12100) loss: 0.048909\n",
      "(Iteration 9421 / 12100) loss: 0.029146\n",
      "(Iteration 9431 / 12100) loss: 0.063245\n",
      "(Iteration 9441 / 12100) loss: 0.034362\n",
      "(Iteration 9451 / 12100) loss: 0.044021\n",
      "(Iteration 9461 / 12100) loss: 0.043607\n",
      "(Iteration 9471 / 12100) loss: 0.039337\n",
      "(Iteration 9481 / 12100) loss: 0.062343\n",
      "(Iteration 9491 / 12100) loss: 0.068283\n",
      "(Iteration 9501 / 12100) loss: 0.045459\n",
      "(Epoch 10 / 5000) train acc: 0.900826; val_acc: 0.770492\n",
      "F1: 0.8055555555555556 \t Precision 0.7073170731707317 \t recall 0.9354838709677419\n",
      "(Iteration 9511 / 12100) loss: 0.093028\n",
      "(Iteration 9521 / 12100) loss: 0.048764\n",
      "(Iteration 9531 / 12100) loss: 0.042480\n",
      "(Iteration 9541 / 12100) loss: 0.132088\n",
      "(Iteration 9551 / 12100) loss: 0.058719\n",
      "(Iteration 9561 / 12100) loss: 0.040737\n",
      "(Iteration 9571 / 12100) loss: 0.103509\n",
      "(Iteration 9581 / 12100) loss: 0.115134\n",
      "(Iteration 9591 / 12100) loss: 0.031937\n",
      "(Iteration 9601 / 12100) loss: 0.029264\n",
      "(Iteration 9611 / 12100) loss: 0.068653\n",
      "(Iteration 9621 / 12100) loss: 0.057218\n",
      "(Iteration 9631 / 12100) loss: 0.052667\n",
      "(Iteration 9641 / 12100) loss: 0.056040\n",
      "(Iteration 9651 / 12100) loss: 0.074792\n",
      "(Iteration 9661 / 12100) loss: 0.064300\n",
      "(Iteration 9671 / 12100) loss: 0.052181\n",
      "(Iteration 9681 / 12100) loss: 0.048537\n",
      "(Iteration 9691 / 12100) loss: 0.034563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 9701 / 12100) loss: 0.034317\n",
      "(Iteration 9711 / 12100) loss: 0.049729\n",
      "(Iteration 9721 / 12100) loss: 0.042583\n",
      "(Iteration 9731 / 12100) loss: 0.064069\n",
      "(Iteration 9741 / 12100) loss: 0.034973\n",
      "(Iteration 9751 / 12100) loss: 0.051655\n",
      "(Iteration 9761 / 12100) loss: 0.035866\n",
      "(Iteration 9771 / 12100) loss: 0.032673\n",
      "(Iteration 9781 / 12100) loss: 0.051227\n",
      "(Iteration 9791 / 12100) loss: 0.029908\n",
      "(Iteration 9801 / 12100) loss: 0.071682\n",
      "(Iteration 9811 / 12100) loss: 0.036192\n",
      "(Iteration 9821 / 12100) loss: 0.028844\n",
      "(Iteration 9831 / 12100) loss: 0.037810\n",
      "(Iteration 9841 / 12100) loss: 0.040766\n",
      "(Iteration 9851 / 12100) loss: 0.040767\n",
      "(Iteration 9861 / 12100) loss: 0.039320\n",
      "(Iteration 9871 / 12100) loss: 0.065920\n",
      "(Iteration 9881 / 12100) loss: 0.132457\n",
      "(Iteration 9891 / 12100) loss: 0.072702\n",
      "(Iteration 9901 / 12100) loss: 0.053918\n",
      "(Iteration 9911 / 12100) loss: 0.038022\n",
      "(Iteration 9921 / 12100) loss: 0.074115\n",
      "(Iteration 9931 / 12100) loss: 0.038569\n",
      "(Iteration 9941 / 12100) loss: 0.036703\n",
      "(Iteration 9951 / 12100) loss: 0.104522\n",
      "(Iteration 9961 / 12100) loss: 0.059043\n",
      "(Iteration 9971 / 12100) loss: 0.035579\n",
      "(Iteration 9981 / 12100) loss: 0.040908\n",
      "(Iteration 9991 / 12100) loss: 0.126032\n",
      "(Iteration 10001 / 12100) loss: 0.100846\n",
      "(Epoch 11 / 5000) train acc: 0.867769; val_acc: 0.737705\n",
      "F1: 0.7777777777777778 \t Precision 0.6829268292682927 \t recall 0.9032258064516129\n",
      "(Iteration 10011 / 12100) loss: 0.068113\n",
      "(Iteration 10021 / 12100) loss: 0.034018\n",
      "(Iteration 10031 / 12100) loss: 0.062630\n",
      "(Iteration 10041 / 12100) loss: 0.077188\n",
      "(Iteration 10051 / 12100) loss: 0.068198\n",
      "(Iteration 10061 / 12100) loss: 0.037610\n",
      "(Iteration 10071 / 12100) loss: 0.085153\n",
      "(Iteration 10081 / 12100) loss: 0.028136\n",
      "(Iteration 10091 / 12100) loss: 0.044362\n",
      "(Iteration 10101 / 12100) loss: 0.054928\n",
      "(Iteration 10111 / 12100) loss: 0.031637\n",
      "(Iteration 10121 / 12100) loss: 0.057673\n",
      "(Iteration 10131 / 12100) loss: 0.205264\n",
      "(Iteration 10141 / 12100) loss: 0.028958\n",
      "(Iteration 10151 / 12100) loss: 0.098840\n",
      "(Iteration 10161 / 12100) loss: 0.048751\n",
      "(Iteration 10171 / 12100) loss: 0.044264\n",
      "(Iteration 10181 / 12100) loss: 0.070516\n",
      "(Iteration 10191 / 12100) loss: 0.075083\n",
      "(Iteration 10201 / 12100) loss: 0.034511\n",
      "(Iteration 10211 / 12100) loss: 0.055523\n",
      "(Iteration 10221 / 12100) loss: 0.067824\n",
      "(Iteration 10231 / 12100) loss: 0.034362\n",
      "(Iteration 10241 / 12100) loss: 0.093931\n",
      "(Iteration 10251 / 12100) loss: 0.043852\n",
      "(Iteration 10261 / 12100) loss: 0.069884\n",
      "(Iteration 10271 / 12100) loss: 0.041023\n",
      "(Iteration 10281 / 12100) loss: 0.063000\n",
      "(Iteration 10291 / 12100) loss: 0.031408\n",
      "(Iteration 10301 / 12100) loss: 0.041627\n",
      "(Iteration 10311 / 12100) loss: 0.046156\n",
      "(Iteration 10321 / 12100) loss: 0.041125\n",
      "(Iteration 10331 / 12100) loss: 0.170366\n",
      "(Iteration 10341 / 12100) loss: 0.070220\n",
      "(Iteration 10351 / 12100) loss: 0.042212\n",
      "(Iteration 10361 / 12100) loss: 0.048338\n",
      "(Iteration 10371 / 12100) loss: 0.042348\n",
      "(Iteration 10381 / 12100) loss: 0.045324\n",
      "(Iteration 10391 / 12100) loss: 0.079682\n",
      "(Iteration 10401 / 12100) loss: 0.034249\n",
      "(Iteration 10411 / 12100) loss: 0.073329\n",
      "(Iteration 10421 / 12100) loss: 0.092656\n",
      "(Iteration 10431 / 12100) loss: 0.027089\n",
      "(Iteration 10441 / 12100) loss: 0.144192\n",
      "(Iteration 10451 / 12100) loss: 0.032496\n",
      "(Iteration 10461 / 12100) loss: 0.060455\n",
      "(Iteration 10471 / 12100) loss: 0.036031\n",
      "(Iteration 10481 / 12100) loss: 0.035728\n",
      "(Iteration 10491 / 12100) loss: 0.029455\n",
      "(Iteration 10501 / 12100) loss: 0.084390\n",
      "(Epoch 11 / 5000) train acc: 0.863636; val_acc: 0.754098\n",
      "F1: 0.7945205479452055 \t Precision 0.6904761904761905 \t recall 0.9354838709677419\n",
      "(Iteration 10511 / 12100) loss: 0.031647\n",
      "(Iteration 10521 / 12100) loss: 0.029891\n",
      "(Iteration 10531 / 12100) loss: 0.060344\n",
      "(Iteration 10541 / 12100) loss: 0.046232\n",
      "(Iteration 10551 / 12100) loss: 0.084625\n",
      "(Iteration 10561 / 12100) loss: 0.130432\n",
      "(Iteration 10571 / 12100) loss: 0.081727\n",
      "(Iteration 10581 / 12100) loss: 0.057867\n",
      "(Iteration 10591 / 12100) loss: 0.031772\n",
      "(Iteration 10601 / 12100) loss: 0.029020\n",
      "(Iteration 10611 / 12100) loss: 0.028636\n",
      "(Iteration 10621 / 12100) loss: 0.127672\n",
      "(Iteration 10631 / 12100) loss: 0.036249\n",
      "(Iteration 10641 / 12100) loss: 0.028338\n",
      "(Iteration 10651 / 12100) loss: 0.060284\n",
      "(Iteration 10661 / 12100) loss: 0.042100\n",
      "(Iteration 10671 / 12100) loss: 0.037997\n",
      "(Iteration 10681 / 12100) loss: 0.031755\n",
      "(Iteration 10691 / 12100) loss: 0.093582\n",
      "(Iteration 10701 / 12100) loss: 0.048513\n",
      "(Iteration 10711 / 12100) loss: 0.050914\n",
      "(Iteration 10721 / 12100) loss: 0.075480\n",
      "(Iteration 10731 / 12100) loss: 0.035697\n",
      "(Iteration 10741 / 12100) loss: 0.114590\n",
      "(Iteration 10751 / 12100) loss: 0.066980\n",
      "(Iteration 10761 / 12100) loss: 0.088263\n",
      "(Iteration 10771 / 12100) loss: 0.042513\n",
      "(Iteration 10781 / 12100) loss: 0.038027\n",
      "(Iteration 10791 / 12100) loss: 0.076137\n",
      "(Iteration 10801 / 12100) loss: 0.070201\n",
      "(Iteration 10811 / 12100) loss: 0.047381\n",
      "(Iteration 10821 / 12100) loss: 0.076406\n",
      "(Iteration 10831 / 12100) loss: 0.054982\n",
      "(Iteration 10841 / 12100) loss: 0.040576\n",
      "(Iteration 10851 / 12100) loss: 0.172563\n",
      "(Iteration 10861 / 12100) loss: 0.042698\n",
      "(Iteration 10871 / 12100) loss: 0.039295\n",
      "(Iteration 10881 / 12100) loss: 0.029231\n",
      "(Iteration 10891 / 12100) loss: 0.051729\n",
      "(Iteration 10901 / 12100) loss: 0.088196\n",
      "(Iteration 10911 / 12100) loss: 0.078205\n",
      "(Iteration 10921 / 12100) loss: 0.072133\n",
      "(Iteration 10931 / 12100) loss: 0.030337\n",
      "(Iteration 10941 / 12100) loss: 0.068104\n",
      "(Iteration 10951 / 12100) loss: 0.042506\n",
      "(Iteration 10961 / 12100) loss: 0.061177\n",
      "(Iteration 10971 / 12100) loss: 0.058499\n",
      "(Iteration 10981 / 12100) loss: 0.054364\n",
      "(Iteration 10991 / 12100) loss: 0.036953\n",
      "(Iteration 11001 / 12100) loss: 0.045358\n",
      "(Epoch 12 / 5000) train acc: 0.900826; val_acc: 0.754098\n",
      "F1: 0.7887323943661972 \t Precision 0.7 \t recall 0.9032258064516129\n",
      "(Iteration 11011 / 12100) loss: 0.031898\n",
      "(Iteration 11021 / 12100) loss: 0.073336\n",
      "(Iteration 11031 / 12100) loss: 0.027160\n",
      "(Iteration 11041 / 12100) loss: 0.037717\n",
      "(Iteration 11051 / 12100) loss: 0.043261\n",
      "(Iteration 11061 / 12100) loss: 0.039188\n",
      "(Iteration 11071 / 12100) loss: 0.029311\n",
      "(Iteration 11081 / 12100) loss: 0.095020\n",
      "(Iteration 11091 / 12100) loss: 0.031234\n",
      "(Iteration 11101 / 12100) loss: 0.031830\n",
      "(Iteration 11111 / 12100) loss: 0.049722\n",
      "(Iteration 11121 / 12100) loss: 0.039757\n",
      "(Iteration 11131 / 12100) loss: 0.043696\n",
      "(Iteration 11141 / 12100) loss: 0.039435\n",
      "(Iteration 11151 / 12100) loss: 0.094263\n",
      "(Iteration 11161 / 12100) loss: 0.030788\n",
      "(Iteration 11171 / 12100) loss: 0.031305\n",
      "(Iteration 11181 / 12100) loss: 0.065252\n",
      "(Iteration 11191 / 12100) loss: 0.028471\n",
      "(Iteration 11201 / 12100) loss: 0.028663\n",
      "(Iteration 11211 / 12100) loss: 0.090279\n",
      "(Iteration 11221 / 12100) loss: 0.032275\n",
      "(Iteration 11231 / 12100) loss: 0.072010\n",
      "(Iteration 11241 / 12100) loss: 0.037072\n",
      "(Iteration 11251 / 12100) loss: 0.039251\n",
      "(Iteration 11261 / 12100) loss: 0.091936\n",
      "(Iteration 11271 / 12100) loss: 0.134770\n",
      "(Iteration 11281 / 12100) loss: 0.029809\n",
      "(Iteration 11291 / 12100) loss: 0.043502\n",
      "(Iteration 11301 / 12100) loss: 0.033235\n",
      "(Iteration 11311 / 12100) loss: 0.049928\n",
      "(Iteration 11321 / 12100) loss: 0.030001\n",
      "(Iteration 11331 / 12100) loss: 0.054470\n",
      "(Iteration 11341 / 12100) loss: 0.039520\n",
      "(Iteration 11351 / 12100) loss: 0.060262\n",
      "(Iteration 11361 / 12100) loss: 0.067416\n",
      "(Iteration 11371 / 12100) loss: 0.037221\n",
      "(Iteration 11381 / 12100) loss: 0.036947\n",
      "(Iteration 11391 / 12100) loss: 0.028984\n",
      "(Iteration 11401 / 12100) loss: 0.044086\n",
      "(Iteration 11411 / 12100) loss: 0.031916\n",
      "(Iteration 11421 / 12100) loss: 0.100020\n",
      "(Iteration 11431 / 12100) loss: 0.028124\n",
      "(Iteration 11441 / 12100) loss: 0.141712\n",
      "(Iteration 11451 / 12100) loss: 0.033307\n",
      "(Iteration 11461 / 12100) loss: 0.107813\n",
      "(Iteration 11471 / 12100) loss: 0.044872\n",
      "(Iteration 11481 / 12100) loss: 0.041892\n",
      "(Iteration 11491 / 12100) loss: 0.060428\n",
      "(Iteration 11501 / 12100) loss: 0.282986\n",
      "(Epoch 12 / 5000) train acc: 0.896694; val_acc: 0.770492\n",
      "F1: 0.8055555555555556 \t Precision 0.7073170731707317 \t recall 0.9354838709677419\n",
      "(Iteration 11511 / 12100) loss: 0.058342\n",
      "(Iteration 11521 / 12100) loss: 0.045146\n",
      "(Iteration 11531 / 12100) loss: 0.045359\n",
      "(Iteration 11541 / 12100) loss: 0.063123\n",
      "(Iteration 11551 / 12100) loss: 0.049156\n",
      "(Iteration 11561 / 12100) loss: 0.027007\n",
      "(Iteration 11571 / 12100) loss: 0.027713\n",
      "(Iteration 11581 / 12100) loss: 0.072067\n",
      "(Iteration 11591 / 12100) loss: 0.095595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 11601 / 12100) loss: 0.116199\n",
      "(Iteration 11611 / 12100) loss: 0.096532\n",
      "(Iteration 11621 / 12100) loss: 0.043105\n",
      "(Iteration 11631 / 12100) loss: 0.044857\n",
      "(Iteration 11641 / 12100) loss: 0.125632\n",
      "(Iteration 11651 / 12100) loss: 0.132823\n",
      "(Iteration 11661 / 12100) loss: 0.031401\n",
      "(Iteration 11671 / 12100) loss: 0.031183\n",
      "(Iteration 11681 / 12100) loss: 0.050724\n",
      "(Iteration 11691 / 12100) loss: 0.105060\n",
      "(Iteration 11701 / 12100) loss: 0.088330\n",
      "(Iteration 11711 / 12100) loss: 0.044411\n",
      "(Iteration 11721 / 12100) loss: 0.134824\n",
      "(Iteration 11731 / 12100) loss: 0.064385\n",
      "(Iteration 11741 / 12100) loss: 0.089800\n",
      "(Iteration 11751 / 12100) loss: 0.043020\n",
      "(Iteration 11761 / 12100) loss: 0.030042\n",
      "(Iteration 11771 / 12100) loss: 0.044897\n",
      "(Iteration 11781 / 12100) loss: 0.043266\n",
      "(Iteration 11791 / 12100) loss: 0.032967\n",
      "(Iteration 11801 / 12100) loss: 0.097281\n",
      "(Iteration 11811 / 12100) loss: 0.099911\n",
      "(Iteration 11821 / 12100) loss: 0.042557\n",
      "(Iteration 11831 / 12100) loss: 0.103593\n",
      "(Iteration 11841 / 12100) loss: 0.044743\n",
      "(Iteration 11851 / 12100) loss: 0.036322\n",
      "(Iteration 11861 / 12100) loss: 0.039626\n",
      "(Iteration 11871 / 12100) loss: 0.073424\n",
      "(Iteration 11881 / 12100) loss: 0.047532\n",
      "(Iteration 11891 / 12100) loss: 0.094863\n",
      "(Iteration 11901 / 12100) loss: 0.066648\n",
      "(Iteration 11911 / 12100) loss: 0.042034\n",
      "(Iteration 11921 / 12100) loss: 0.037353\n",
      "(Iteration 11931 / 12100) loss: 0.046261\n",
      "(Iteration 11941 / 12100) loss: 0.063695\n",
      "(Iteration 11951 / 12100) loss: 0.040193\n",
      "(Iteration 11961 / 12100) loss: 0.035215\n",
      "(Iteration 11971 / 12100) loss: 0.031952\n",
      "(Iteration 11981 / 12100) loss: 0.029234\n",
      "(Iteration 11991 / 12100) loss: 0.037363\n",
      "(Iteration 12001 / 12100) loss: 0.030389\n",
      "(Epoch 13 / 5000) train acc: 0.904959; val_acc: 0.803279\n",
      "F1: 0.8285714285714285 \t Precision 0.7435897435897436 \t recall 0.9354838709677419\n",
      "(Iteration 12011 / 12100) loss: 0.051124\n",
      "(Iteration 12021 / 12100) loss: 0.080760\n",
      "(Iteration 12031 / 12100) loss: 0.059014\n",
      "(Iteration 12041 / 12100) loss: 0.030523\n",
      "(Iteration 12051 / 12100) loss: 0.118842\n",
      "(Iteration 12061 / 12100) loss: 0.039762\n",
      "(Iteration 12071 / 12100) loss: 0.040606\n",
      "(Iteration 12081 / 12100) loss: 0.028339\n",
      "(Iteration 12091 / 12100) loss: 0.052503\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FullyConnectedNet([100, 100, 100, 100], input_dim=13, num_classes=2, dropout=0.5, reg=0.001)\n",
    "\n",
    "\n",
    "trainer = trainer.Trainer(model, data, num_epochs=5000)\n",
    "trainer.train()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(trainer.get_losshistory(), 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(trainer.get_train_acc_history(), '-o')\n",
    "plt.plot(trainer.get_val_acc_history(), '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_csv(\"data/heart.csv\")\n",
    "plt.matshow(data.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
