{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 14)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143\n",
       "2     87\n",
       "1     50\n",
       "3     23\n",
       "Name: cp, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    165\n",
       "0    138\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /home/daniel/.local/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/daniel/.local/lib/python3.6/site-packages (from sklearn) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/daniel/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.15.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /home/daniel/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from networks.NeuralNet import FullyConnectedNet\n",
    "from layers import *\n",
    "import trainer\n",
    "import sys\n",
    "!{sys.executable} -m pip install --user sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:16: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  app.launch_new_instance()\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:19: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(242, 13)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train = train['target']\n",
    "#X_train = train.drop(['target'],  axis =1)\n",
    "#y_test = test['target']\n",
    "#X_test = test.drop(['target'],  axis =1)\n",
    "\n",
    "y = data['target']\n",
    "x = data.drop(['target'],  axis =1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y, test_size = 0.2,random_state = 5)\n",
    "\n",
    "X_train, _, _ = feature_normalize(X_train)\n",
    "X_test, _, _ = feature_normalize(X_test)\n",
    "\n",
    "data = {}\n",
    "data['X_train'] = X_train.as_matrix()\n",
    "data['y_train'] = y_train.values\n",
    "data['y_val'] = y_test.values\n",
    "data['X_val'] = X_test.as_matrix()\n",
    "\n",
    "data['X_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07030624, -1.50674161, -0.97227182, ..., -0.65683133,\n",
       "        -0.69043052, -0.49747423],\n",
       "       [-1.2729129 , -1.50674161, -0.97227182, ..., -0.65683133,\n",
       "        -0.69043052,  1.15168692],\n",
       "       [-0.15356362,  0.6636838 ,  0.97227182, ...,  0.98186126,\n",
       "         2.31108179, -0.49747423],\n",
       "       ...,\n",
       "       [-0.93710811, -1.50674161,  0.        , ...,  0.98186126,\n",
       "        -0.69043052, -0.49747423],\n",
       "       [-1.49678275,  0.6636838 , -0.97227182, ...,  0.98186126,\n",
       "        -0.69043052,  1.15168692],\n",
       "       [ 0.51804595,  0.6636838 , -0.97227182, ..., -0.65683133,\n",
       "         0.31007359,  1.15168692]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer set to <function adam at 0x7fcfa41d7730>.\n",
      "(Iteration 1 / 12100) loss: 0.693141\n",
      "(Epoch 1 / 5000) train acc: 0.297521; val_acc: 0.360656\n",
      "F1: 0.23529411764705882 \t Precision 0.3 \t recall 0.1935483870967742\n",
      "(Iteration 11 / 12100) loss: 0.359024\n",
      "(Iteration 21 / 12100) loss: 0.288331\n",
      "(Iteration 31 / 12100) loss: 0.277109\n",
      "(Iteration 41 / 12100) loss: 0.295956\n",
      "(Iteration 51 / 12100) loss: 0.211457\n",
      "(Iteration 61 / 12100) loss: 0.188400\n",
      "(Iteration 71 / 12100) loss: 0.221418\n",
      "(Iteration 81 / 12100) loss: 0.214029\n",
      "(Iteration 91 / 12100) loss: 0.222724\n",
      "(Iteration 101 / 12100) loss: 0.218781\n",
      "(Iteration 111 / 12100) loss: 0.216269\n",
      "(Iteration 121 / 12100) loss: 0.174030\n",
      "(Iteration 131 / 12100) loss: 0.125302\n",
      "(Iteration 141 / 12100) loss: 0.160870\n",
      "(Iteration 151 / 12100) loss: 0.181086\n",
      "(Iteration 161 / 12100) loss: 0.123378\n",
      "(Iteration 171 / 12100) loss: 0.159756\n",
      "(Iteration 181 / 12100) loss: 0.147518\n",
      "(Iteration 191 / 12100) loss: 0.144327\n",
      "(Iteration 201 / 12100) loss: 0.197797\n",
      "(Iteration 211 / 12100) loss: 0.125991\n",
      "(Iteration 221 / 12100) loss: 0.134843\n",
      "(Iteration 231 / 12100) loss: 0.173100\n",
      "(Iteration 241 / 12100) loss: 0.069847\n",
      "(Iteration 251 / 12100) loss: 0.103492\n",
      "(Iteration 261 / 12100) loss: 0.136417\n",
      "(Iteration 271 / 12100) loss: 0.162044\n",
      "(Iteration 281 / 12100) loss: 0.090762\n",
      "(Iteration 291 / 12100) loss: 0.037733\n",
      "(Iteration 301 / 12100) loss: 0.136061\n",
      "(Iteration 311 / 12100) loss: 0.107691\n",
      "(Iteration 321 / 12100) loss: 0.055617\n",
      "(Iteration 331 / 12100) loss: 0.088981\n",
      "(Iteration 341 / 12100) loss: 0.160333\n",
      "(Iteration 351 / 12100) loss: 0.091509\n",
      "(Iteration 361 / 12100) loss: 0.088414\n",
      "(Iteration 371 / 12100) loss: 0.060623\n",
      "(Iteration 381 / 12100) loss: 0.067689\n",
      "(Iteration 391 / 12100) loss: 0.037308\n",
      "(Iteration 401 / 12100) loss: 0.064160\n",
      "(Iteration 411 / 12100) loss: 0.077092\n",
      "(Iteration 421 / 12100) loss: 0.073401\n",
      "(Iteration 431 / 12100) loss: 0.041315\n",
      "(Iteration 441 / 12100) loss: 0.065894\n",
      "(Iteration 451 / 12100) loss: 0.067917\n",
      "(Iteration 461 / 12100) loss: 0.036463\n",
      "(Iteration 471 / 12100) loss: 0.031917\n",
      "(Iteration 481 / 12100) loss: 0.043334\n",
      "(Iteration 491 / 12100) loss: 0.080133\n",
      "(Iteration 501 / 12100) loss: 0.088678\n",
      "(Epoch 1 / 5000) train acc: 0.975207; val_acc: 0.836066\n",
      "F1: 0.84375 \t Precision 0.8181818181818182 \t recall 0.8709677419354839\n",
      "(Iteration 511 / 12100) loss: 0.102826\n",
      "(Iteration 521 / 12100) loss: 0.102119\n",
      "(Iteration 531 / 12100) loss: 0.046665\n",
      "(Iteration 541 / 12100) loss: 0.048345\n",
      "(Iteration 551 / 12100) loss: 0.052965\n",
      "(Iteration 561 / 12100) loss: 0.099361\n",
      "(Iteration 571 / 12100) loss: 0.086436\n",
      "(Iteration 581 / 12100) loss: 0.070935\n",
      "(Iteration 591 / 12100) loss: 0.059175\n",
      "(Iteration 601 / 12100) loss: 0.091511\n",
      "(Iteration 611 / 12100) loss: 0.036665\n",
      "(Iteration 621 / 12100) loss: 0.230179\n",
      "(Iteration 631 / 12100) loss: 0.036601\n",
      "(Iteration 641 / 12100) loss: 0.034071\n",
      "(Iteration 651 / 12100) loss: 0.054415\n",
      "(Iteration 661 / 12100) loss: 0.044553\n",
      "(Iteration 671 / 12100) loss: 0.043658\n",
      "(Iteration 681 / 12100) loss: 0.064235\n",
      "(Iteration 691 / 12100) loss: 0.055236\n",
      "(Iteration 701 / 12100) loss: 0.042341\n",
      "(Iteration 711 / 12100) loss: 0.066483\n",
      "(Iteration 721 / 12100) loss: 0.031418\n",
      "(Iteration 731 / 12100) loss: 0.040112\n",
      "(Iteration 741 / 12100) loss: 0.111159\n",
      "(Iteration 751 / 12100) loss: 0.109046\n",
      "(Iteration 761 / 12100) loss: 0.052114\n",
      "(Iteration 771 / 12100) loss: 0.035302\n",
      "(Iteration 781 / 12100) loss: 0.076079\n",
      "(Iteration 791 / 12100) loss: 0.057004\n",
      "(Iteration 801 / 12100) loss: 0.047114\n",
      "(Iteration 811 / 12100) loss: 0.047064\n",
      "(Iteration 821 / 12100) loss: 0.056218\n",
      "(Iteration 831 / 12100) loss: 0.042617\n",
      "(Iteration 841 / 12100) loss: 0.052590\n",
      "(Iteration 851 / 12100) loss: 0.044485\n",
      "(Iteration 861 / 12100) loss: 0.032085\n",
      "(Iteration 871 / 12100) loss: 0.032047\n",
      "(Iteration 881 / 12100) loss: 0.027192\n",
      "(Iteration 891 / 12100) loss: 0.026344\n",
      "(Iteration 901 / 12100) loss: 0.033548\n",
      "(Iteration 911 / 12100) loss: 0.027183\n",
      "(Iteration 921 / 12100) loss: 0.043961\n",
      "(Iteration 931 / 12100) loss: 0.044073\n",
      "(Iteration 941 / 12100) loss: 0.054649\n",
      "(Iteration 951 / 12100) loss: 0.034643\n",
      "(Iteration 961 / 12100) loss: 0.038490\n",
      "(Iteration 971 / 12100) loss: 0.061723\n",
      "(Iteration 981 / 12100) loss: 0.078505\n",
      "(Iteration 991 / 12100) loss: 0.042382\n",
      "(Iteration 1001 / 12100) loss: 0.130242\n",
      "(Epoch 2 / 5000) train acc: 0.983471; val_acc: 0.852459\n",
      "F1: 0.8571428571428571 \t Precision 0.84375 \t recall 0.8709677419354839\n",
      "(Iteration 1011 / 12100) loss: 0.032805\n",
      "(Iteration 1021 / 12100) loss: 0.082984\n",
      "(Iteration 1031 / 12100) loss: 0.056030\n",
      "(Iteration 1041 / 12100) loss: 0.042608\n",
      "(Iteration 1051 / 12100) loss: 0.026230\n",
      "(Iteration 1061 / 12100) loss: 0.029681\n",
      "(Iteration 1071 / 12100) loss: 0.071736\n",
      "(Iteration 1081 / 12100) loss: 0.024915\n",
      "(Iteration 1091 / 12100) loss: 0.048864\n",
      "(Iteration 1101 / 12100) loss: 0.033142\n",
      "(Iteration 1111 / 12100) loss: 0.072653\n",
      "(Iteration 1121 / 12100) loss: 0.029805\n",
      "(Iteration 1131 / 12100) loss: 0.039373\n",
      "(Iteration 1141 / 12100) loss: 0.033836\n",
      "(Iteration 1151 / 12100) loss: 0.035476\n",
      "(Iteration 1161 / 12100) loss: 0.033007\n",
      "(Iteration 1171 / 12100) loss: 0.055430\n",
      "(Iteration 1181 / 12100) loss: 0.032191\n",
      "(Iteration 1191 / 12100) loss: 0.133127\n",
      "(Iteration 1201 / 12100) loss: 0.045651\n",
      "(Iteration 1211 / 12100) loss: 0.081038\n",
      "(Iteration 1221 / 12100) loss: 0.045148\n",
      "(Iteration 1231 / 12100) loss: 0.044208\n",
      "(Iteration 1241 / 12100) loss: 0.034392\n",
      "(Iteration 1251 / 12100) loss: 0.040489\n",
      "(Iteration 1261 / 12100) loss: 0.034989\n",
      "(Iteration 1271 / 12100) loss: 0.053376\n",
      "(Iteration 1281 / 12100) loss: 0.094398\n",
      "(Iteration 1291 / 12100) loss: 0.037335\n",
      "(Iteration 1301 / 12100) loss: 0.047494\n",
      "(Iteration 1311 / 12100) loss: 0.071635\n",
      "(Iteration 1321 / 12100) loss: 0.064465\n",
      "(Iteration 1331 / 12100) loss: 0.052772\n",
      "(Iteration 1341 / 12100) loss: 0.036615\n",
      "(Iteration 1351 / 12100) loss: 0.033883\n",
      "(Iteration 1361 / 12100) loss: 0.025124\n",
      "(Iteration 1371 / 12100) loss: 0.034538\n",
      "(Iteration 1381 / 12100) loss: 0.044591\n",
      "(Iteration 1391 / 12100) loss: 0.046324\n",
      "(Iteration 1401 / 12100) loss: 0.028413\n",
      "(Iteration 1411 / 12100) loss: 0.032090\n",
      "(Iteration 1421 / 12100) loss: 0.056411\n",
      "(Iteration 1431 / 12100) loss: 0.067255\n",
      "(Iteration 1441 / 12100) loss: 0.029624\n",
      "(Iteration 1451 / 12100) loss: 0.068732\n",
      "(Iteration 1461 / 12100) loss: 0.041606\n",
      "(Iteration 1471 / 12100) loss: 0.027842\n",
      "(Iteration 1481 / 12100) loss: 0.051165\n",
      "(Iteration 1491 / 12100) loss: 0.043731\n",
      "(Iteration 1501 / 12100) loss: 0.028651\n",
      "(Epoch 2 / 5000) train acc: 0.995868; val_acc: 0.819672\n",
      "F1: 0.8307692307692308 \t Precision 0.7941176470588235 \t recall 0.8709677419354839\n",
      "(Iteration 1511 / 12100) loss: 0.026431\n",
      "(Iteration 1521 / 12100) loss: 0.165954\n",
      "(Iteration 1531 / 12100) loss: 0.052297\n",
      "(Iteration 1541 / 12100) loss: 0.083494\n",
      "(Iteration 1551 / 12100) loss: 0.035257\n",
      "(Iteration 1561 / 12100) loss: 0.057539\n",
      "(Iteration 1571 / 12100) loss: 0.056860\n",
      "(Iteration 1581 / 12100) loss: 0.049651\n",
      "(Iteration 1591 / 12100) loss: 0.059087\n",
      "(Iteration 1601 / 12100) loss: 0.029935\n",
      "(Iteration 1611 / 12100) loss: 0.035972\n",
      "(Iteration 1621 / 12100) loss: 0.031614\n",
      "(Iteration 1631 / 12100) loss: 0.034297\n",
      "(Iteration 1641 / 12100) loss: 0.026848\n",
      "(Iteration 1651 / 12100) loss: 0.043094\n",
      "(Iteration 1661 / 12100) loss: 0.034399\n",
      "(Iteration 1671 / 12100) loss: 0.035498\n",
      "(Iteration 1681 / 12100) loss: 0.026896\n",
      "(Iteration 1691 / 12100) loss: 0.051864\n",
      "(Iteration 1701 / 12100) loss: 0.032050\n",
      "(Iteration 1711 / 12100) loss: 0.047198\n",
      "(Iteration 1721 / 12100) loss: 0.030288\n",
      "(Iteration 1731 / 12100) loss: 0.032394\n",
      "(Iteration 1741 / 12100) loss: 0.044360\n",
      "(Iteration 1751 / 12100) loss: 0.061237\n",
      "(Iteration 1761 / 12100) loss: 0.032029\n",
      "(Iteration 1771 / 12100) loss: 0.032084\n",
      "(Iteration 1781 / 12100) loss: 0.032583\n",
      "(Iteration 1791 / 12100) loss: 0.048857\n",
      "(Iteration 1801 / 12100) loss: 0.030413\n",
      "(Iteration 1811 / 12100) loss: 0.035292\n",
      "(Iteration 1821 / 12100) loss: 0.031296\n",
      "(Iteration 1831 / 12100) loss: 0.047746\n",
      "(Iteration 1841 / 12100) loss: 0.046493\n",
      "(Iteration 1851 / 12100) loss: 0.042712\n",
      "(Iteration 1861 / 12100) loss: 0.035411\n",
      "(Iteration 1871 / 12100) loss: 0.041000\n",
      "(Iteration 1881 / 12100) loss: 0.062490\n",
      "(Iteration 1891 / 12100) loss: 0.038813\n",
      "(Iteration 1901 / 12100) loss: 0.052930\n",
      "(Iteration 1911 / 12100) loss: 0.037009\n",
      "(Iteration 1921 / 12100) loss: 0.089921\n",
      "(Iteration 1931 / 12100) loss: 0.045893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1941 / 12100) loss: 0.059750\n",
      "(Iteration 1951 / 12100) loss: 0.037364\n",
      "(Iteration 1961 / 12100) loss: 0.039407\n",
      "(Iteration 1971 / 12100) loss: 0.114912\n",
      "(Iteration 1981 / 12100) loss: 0.027811\n",
      "(Iteration 1991 / 12100) loss: 0.030630\n",
      "(Iteration 2001 / 12100) loss: 0.037495\n",
      "(Epoch 3 / 5000) train acc: 0.991736; val_acc: 0.836066\n",
      "F1: 0.84375 \t Precision 0.8181818181818182 \t recall 0.8709677419354839\n",
      "(Iteration 2011 / 12100) loss: 0.039162\n",
      "(Iteration 2021 / 12100) loss: 0.035666\n",
      "(Iteration 2031 / 12100) loss: 0.037544\n",
      "(Iteration 2041 / 12100) loss: 0.030901\n",
      "(Iteration 2051 / 12100) loss: 0.055266\n",
      "(Iteration 2061 / 12100) loss: 0.040868\n",
      "(Iteration 2071 / 12100) loss: 0.031974\n",
      "(Iteration 2081 / 12100) loss: 0.029529\n",
      "(Iteration 2091 / 12100) loss: 0.044105\n",
      "(Iteration 2101 / 12100) loss: 0.038778\n",
      "(Iteration 2111 / 12100) loss: 0.062176\n",
      "(Iteration 2121 / 12100) loss: 0.076572\n",
      "(Iteration 2131 / 12100) loss: 0.044907\n",
      "(Iteration 2141 / 12100) loss: 0.029438\n",
      "(Iteration 2151 / 12100) loss: 0.046344\n",
      "(Iteration 2161 / 12100) loss: 0.027832\n",
      "(Iteration 2171 / 12100) loss: 0.069041\n",
      "(Iteration 2181 / 12100) loss: 0.029312\n",
      "(Iteration 2191 / 12100) loss: 0.028903\n",
      "(Iteration 2201 / 12100) loss: 0.028426\n",
      "(Iteration 2211 / 12100) loss: 0.075420\n",
      "(Iteration 2221 / 12100) loss: 0.040787\n",
      "(Iteration 2231 / 12100) loss: 0.038351\n",
      "(Iteration 2241 / 12100) loss: 0.032892\n",
      "(Iteration 2251 / 12100) loss: 0.034363\n",
      "(Iteration 2261 / 12100) loss: 0.038190\n",
      "(Iteration 2271 / 12100) loss: 0.033890\n",
      "(Iteration 2281 / 12100) loss: 0.038720\n",
      "(Iteration 2291 / 12100) loss: 0.027229\n",
      "(Iteration 2301 / 12100) loss: 0.030518\n",
      "(Iteration 2311 / 12100) loss: 0.065356\n",
      "(Iteration 2321 / 12100) loss: 0.035507\n",
      "(Iteration 2331 / 12100) loss: 0.036834\n",
      "(Iteration 2341 / 12100) loss: 0.036178\n",
      "(Iteration 2351 / 12100) loss: 0.042902\n",
      "(Iteration 2361 / 12100) loss: 0.031878\n",
      "(Iteration 2371 / 12100) loss: 0.044289\n",
      "(Iteration 2381 / 12100) loss: 0.064830\n",
      "(Iteration 2391 / 12100) loss: 0.031926\n",
      "(Iteration 2401 / 12100) loss: 0.064923\n",
      "(Iteration 2411 / 12100) loss: 0.039902\n",
      "(Iteration 2421 / 12100) loss: 0.037618\n",
      "(Iteration 2431 / 12100) loss: 0.038696\n",
      "(Iteration 2441 / 12100) loss: 0.042451\n",
      "(Iteration 2451 / 12100) loss: 0.050729\n",
      "(Iteration 2461 / 12100) loss: 0.030135\n",
      "(Iteration 2471 / 12100) loss: 0.036436\n",
      "(Iteration 2481 / 12100) loss: 0.030974\n",
      "(Iteration 2491 / 12100) loss: 0.033027\n",
      "(Iteration 2501 / 12100) loss: 0.057866\n",
      "(Epoch 3 / 5000) train acc: 0.995868; val_acc: 0.852459\n",
      "F1: 0.8524590163934426 \t Precision 0.8666666666666667 \t recall 0.8387096774193549\n",
      "(Iteration 2511 / 12100) loss: 0.031154\n",
      "(Iteration 2521 / 12100) loss: 0.087581\n",
      "(Iteration 2531 / 12100) loss: 0.032139\n",
      "(Iteration 2541 / 12100) loss: 0.031543\n",
      "(Iteration 2551 / 12100) loss: 0.028984\n",
      "(Iteration 2561 / 12100) loss: 0.028474\n",
      "(Iteration 2571 / 12100) loss: 0.029015\n",
      "(Iteration 2581 / 12100) loss: 0.031914\n",
      "(Iteration 2591 / 12100) loss: 0.063498\n",
      "(Iteration 2601 / 12100) loss: 0.037821\n",
      "(Iteration 2611 / 12100) loss: 0.037128\n",
      "(Iteration 2621 / 12100) loss: 0.031015\n",
      "(Iteration 2631 / 12100) loss: 0.034333\n",
      "(Iteration 2641 / 12100) loss: 0.040902\n",
      "(Iteration 2651 / 12100) loss: 0.036598\n",
      "(Iteration 2661 / 12100) loss: 0.032110\n",
      "(Iteration 2671 / 12100) loss: 0.038114\n",
      "(Iteration 2681 / 12100) loss: 0.037880\n",
      "(Iteration 2691 / 12100) loss: 0.034950\n",
      "(Iteration 2701 / 12100) loss: 0.049932\n",
      "(Iteration 2711 / 12100) loss: 0.027898\n",
      "(Iteration 2721 / 12100) loss: 0.099359\n",
      "(Iteration 2731 / 12100) loss: 0.044032\n",
      "(Iteration 2741 / 12100) loss: 0.094227\n",
      "(Iteration 2751 / 12100) loss: 0.035867\n",
      "(Iteration 2761 / 12100) loss: 0.076262\n",
      "(Iteration 2771 / 12100) loss: 0.094745\n",
      "(Iteration 2781 / 12100) loss: 0.046636\n",
      "(Iteration 2791 / 12100) loss: 0.035001\n",
      "(Iteration 2801 / 12100) loss: 0.032617\n",
      "(Iteration 2811 / 12100) loss: 0.114771\n",
      "(Iteration 2821 / 12100) loss: 0.032537\n",
      "(Iteration 2831 / 12100) loss: 0.032031\n",
      "(Iteration 2841 / 12100) loss: 0.033228\n",
      "(Iteration 2851 / 12100) loss: 0.030757\n",
      "(Iteration 2861 / 12100) loss: 0.048860\n",
      "(Iteration 2871 / 12100) loss: 0.036132\n",
      "(Iteration 2881 / 12100) loss: 0.053934\n",
      "(Iteration 2891 / 12100) loss: 0.032758\n",
      "(Iteration 2901 / 12100) loss: 0.029587\n",
      "(Iteration 2911 / 12100) loss: 0.123454\n",
      "(Iteration 2921 / 12100) loss: 0.039319\n",
      "(Iteration 2931 / 12100) loss: 0.036444\n",
      "(Iteration 2941 / 12100) loss: 0.048973\n",
      "(Iteration 2951 / 12100) loss: 0.037142\n",
      "(Iteration 2961 / 12100) loss: 0.032682\n",
      "(Iteration 2971 / 12100) loss: 0.042048\n",
      "(Iteration 2981 / 12100) loss: 0.029057\n",
      "(Iteration 2991 / 12100) loss: 0.043475\n",
      "(Iteration 3001 / 12100) loss: 0.054978\n",
      "(Epoch 4 / 5000) train acc: 0.987603; val_acc: 0.819672\n",
      "F1: 0.835820895522388 \t Precision 0.7777777777777778 \t recall 0.9032258064516129\n",
      "(Iteration 3011 / 12100) loss: 0.039545\n",
      "(Iteration 3021 / 12100) loss: 0.032821\n",
      "(Iteration 3031 / 12100) loss: 0.052108\n",
      "(Iteration 3041 / 12100) loss: 0.054528\n",
      "(Iteration 3051 / 12100) loss: 0.034638\n",
      "(Iteration 3061 / 12100) loss: 0.130845\n",
      "(Iteration 3071 / 12100) loss: 0.035641\n",
      "(Iteration 3081 / 12100) loss: 0.028817\n",
      "(Iteration 3091 / 12100) loss: 0.032528\n",
      "(Iteration 3101 / 12100) loss: 0.037333\n",
      "(Iteration 3111 / 12100) loss: 0.028588\n",
      "(Iteration 3121 / 12100) loss: 0.028869\n",
      "(Iteration 3131 / 12100) loss: 0.031605\n",
      "(Iteration 3141 / 12100) loss: 0.034293\n",
      "(Iteration 3151 / 12100) loss: 0.035816\n",
      "(Iteration 3161 / 12100) loss: 0.046732\n",
      "(Iteration 3171 / 12100) loss: 0.035177\n",
      "(Iteration 3181 / 12100) loss: 0.030703\n",
      "(Iteration 3191 / 12100) loss: 0.034479\n",
      "(Iteration 3201 / 12100) loss: 0.055313\n",
      "(Iteration 3211 / 12100) loss: 0.036243\n",
      "(Iteration 3221 / 12100) loss: 0.043972\n",
      "(Iteration 3231 / 12100) loss: 0.034560\n",
      "(Iteration 3241 / 12100) loss: 0.035378\n",
      "(Iteration 3251 / 12100) loss: 0.037273\n",
      "(Iteration 3261 / 12100) loss: 0.036283\n",
      "(Iteration 3271 / 12100) loss: 0.067372\n",
      "(Iteration 3281 / 12100) loss: 0.047655\n",
      "(Iteration 3291 / 12100) loss: 0.035743\n",
      "(Iteration 3301 / 12100) loss: 0.033480\n",
      "(Iteration 3311 / 12100) loss: 0.048233\n",
      "(Iteration 3321 / 12100) loss: 0.030177\n",
      "(Iteration 3331 / 12100) loss: 0.032961\n",
      "(Iteration 3341 / 12100) loss: 0.031476\n",
      "(Iteration 3351 / 12100) loss: 0.036944\n",
      "(Iteration 3361 / 12100) loss: 0.046095\n",
      "(Iteration 3371 / 12100) loss: 0.030324\n",
      "(Iteration 3381 / 12100) loss: 0.028927\n",
      "(Iteration 3391 / 12100) loss: 0.034726\n",
      "(Iteration 3401 / 12100) loss: 0.032188\n",
      "(Iteration 3411 / 12100) loss: 0.043423\n",
      "(Iteration 3421 / 12100) loss: 0.030107\n",
      "(Iteration 3431 / 12100) loss: 0.033215\n",
      "(Iteration 3441 / 12100) loss: 0.095783\n",
      "(Iteration 3451 / 12100) loss: 0.043107\n",
      "(Iteration 3461 / 12100) loss: 0.029736\n",
      "(Iteration 3471 / 12100) loss: 0.030697\n",
      "(Iteration 3481 / 12100) loss: 0.053160\n",
      "(Iteration 3491 / 12100) loss: 0.033138\n",
      "(Iteration 3501 / 12100) loss: 0.031278\n",
      "(Epoch 4 / 5000) train acc: 0.987603; val_acc: 0.852459\n",
      "F1: 0.8615384615384616 \t Precision 0.8235294117647058 \t recall 0.9032258064516129\n",
      "(Iteration 3511 / 12100) loss: 0.030040\n",
      "(Iteration 3521 / 12100) loss: 0.050595\n",
      "(Iteration 3531 / 12100) loss: 0.071864\n",
      "(Iteration 3541 / 12100) loss: 0.031656\n",
      "(Iteration 3551 / 12100) loss: 0.031152\n",
      "(Iteration 3561 / 12100) loss: 0.071790\n",
      "(Iteration 3571 / 12100) loss: 0.032860\n",
      "(Iteration 3581 / 12100) loss: 0.031045\n",
      "(Iteration 3591 / 12100) loss: 0.052807\n",
      "(Iteration 3601 / 12100) loss: 0.030086\n",
      "(Iteration 3611 / 12100) loss: 0.033973\n",
      "(Iteration 3621 / 12100) loss: 0.037324\n",
      "(Iteration 3631 / 12100) loss: 0.031377\n",
      "(Iteration 3641 / 12100) loss: 0.045549\n",
      "(Iteration 3651 / 12100) loss: 0.039521\n",
      "(Iteration 3661 / 12100) loss: 0.036732\n",
      "(Iteration 3671 / 12100) loss: 0.033165\n",
      "(Iteration 3681 / 12100) loss: 0.041831\n",
      "(Iteration 3691 / 12100) loss: 0.048592\n",
      "(Iteration 3701 / 12100) loss: 0.029993\n",
      "(Iteration 3711 / 12100) loss: 0.071364\n",
      "(Iteration 3721 / 12100) loss: 0.029666\n",
      "(Iteration 3731 / 12100) loss: 0.030770\n",
      "(Iteration 3741 / 12100) loss: 0.033130\n",
      "(Iteration 3751 / 12100) loss: 0.054128\n",
      "(Iteration 3761 / 12100) loss: 0.099336\n",
      "(Iteration 3771 / 12100) loss: 0.040876\n",
      "(Iteration 3781 / 12100) loss: 0.033984\n",
      "(Iteration 3791 / 12100) loss: 0.051289\n",
      "(Iteration 3801 / 12100) loss: 0.029401\n",
      "(Iteration 3811 / 12100) loss: 0.032187\n",
      "(Iteration 3821 / 12100) loss: 0.034354\n",
      "(Iteration 3831 / 12100) loss: 0.032562\n",
      "(Iteration 3841 / 12100) loss: 0.049924\n",
      "(Iteration 3851 / 12100) loss: 0.040266\n",
      "(Iteration 3861 / 12100) loss: 0.037171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 3871 / 12100) loss: 0.029328\n",
      "(Iteration 3881 / 12100) loss: 0.036171\n",
      "(Iteration 3891 / 12100) loss: 0.067927\n",
      "(Iteration 3901 / 12100) loss: 0.030996\n",
      "(Iteration 3911 / 12100) loss: 0.037662\n",
      "(Iteration 3921 / 12100) loss: 0.085661\n",
      "(Iteration 3931 / 12100) loss: 0.029107\n",
      "(Iteration 3941 / 12100) loss: 0.043574\n",
      "(Iteration 3951 / 12100) loss: 0.035999\n",
      "(Iteration 3961 / 12100) loss: 0.030885\n",
      "(Iteration 3971 / 12100) loss: 0.038607\n",
      "(Iteration 3981 / 12100) loss: 0.028407\n",
      "(Iteration 3991 / 12100) loss: 0.030674\n",
      "(Iteration 4001 / 12100) loss: 0.029247\n",
      "(Epoch 5 / 5000) train acc: 0.971074; val_acc: 0.868852\n",
      "F1: 0.8787878787878788 \t Precision 0.8285714285714286 \t recall 0.9354838709677419\n",
      "(Iteration 4011 / 12100) loss: 0.036487\n",
      "(Iteration 4021 / 12100) loss: 0.033937\n",
      "(Iteration 4031 / 12100) loss: 0.035229\n",
      "(Iteration 4041 / 12100) loss: 0.039541\n",
      "(Iteration 4051 / 12100) loss: 0.036783\n",
      "(Iteration 4061 / 12100) loss: 0.035249\n",
      "(Iteration 4071 / 12100) loss: 0.049975\n",
      "(Iteration 4081 / 12100) loss: 0.046273\n",
      "(Iteration 4091 / 12100) loss: 0.029954\n",
      "(Iteration 4101 / 12100) loss: 0.034978\n",
      "(Iteration 4111 / 12100) loss: 0.038209\n",
      "(Iteration 4121 / 12100) loss: 0.040169\n",
      "(Iteration 4131 / 12100) loss: 0.030134\n",
      "(Iteration 4141 / 12100) loss: 0.105346\n",
      "(Iteration 4151 / 12100) loss: 0.061140\n",
      "(Iteration 4161 / 12100) loss: 0.035019\n",
      "(Iteration 4171 / 12100) loss: 0.047409\n",
      "(Iteration 4181 / 12100) loss: 0.035620\n",
      "(Iteration 4191 / 12100) loss: 0.031623\n",
      "(Iteration 4201 / 12100) loss: 0.033009\n",
      "(Iteration 4211 / 12100) loss: 0.033134\n",
      "(Iteration 4221 / 12100) loss: 0.032177\n",
      "(Iteration 4231 / 12100) loss: 0.066397\n",
      "(Iteration 4241 / 12100) loss: 0.060670\n",
      "(Iteration 4251 / 12100) loss: 0.041453\n",
      "(Iteration 4261 / 12100) loss: 0.032571\n",
      "(Iteration 4271 / 12100) loss: 0.045538\n",
      "(Iteration 4281 / 12100) loss: 0.060721\n",
      "(Iteration 4291 / 12100) loss: 0.036293\n",
      "(Iteration 4301 / 12100) loss: 0.033799\n",
      "(Iteration 4311 / 12100) loss: 0.031780\n",
      "(Iteration 4321 / 12100) loss: 0.041771\n",
      "(Iteration 4331 / 12100) loss: 0.035885\n",
      "(Iteration 4341 / 12100) loss: 0.043465\n",
      "(Iteration 4351 / 12100) loss: 0.030208\n",
      "(Iteration 4361 / 12100) loss: 0.030626\n",
      "(Iteration 4371 / 12100) loss: 0.069168\n",
      "(Iteration 4381 / 12100) loss: 0.045038\n",
      "(Iteration 4391 / 12100) loss: 0.036109\n",
      "(Iteration 4401 / 12100) loss: 0.035714\n",
      "(Iteration 4411 / 12100) loss: 0.036475\n",
      "(Iteration 4421 / 12100) loss: 0.032347\n",
      "(Iteration 4431 / 12100) loss: 0.037179\n",
      "(Iteration 4441 / 12100) loss: 0.032265\n",
      "(Iteration 4451 / 12100) loss: 0.031692\n",
      "(Iteration 4461 / 12100) loss: 0.030478\n",
      "(Iteration 4471 / 12100) loss: 0.052435\n",
      "(Iteration 4481 / 12100) loss: 0.034485\n",
      "(Iteration 4491 / 12100) loss: 0.037572\n",
      "(Iteration 4501 / 12100) loss: 0.047997\n",
      "(Epoch 5 / 5000) train acc: 0.987603; val_acc: 0.901639\n",
      "F1: 0.90625 \t Precision 0.8787878787878788 \t recall 0.9354838709677419\n",
      "(Iteration 4511 / 12100) loss: 0.046021\n",
      "(Iteration 4521 / 12100) loss: 0.056351\n",
      "(Iteration 4531 / 12100) loss: 0.031390\n",
      "(Iteration 4541 / 12100) loss: 0.049546\n",
      "(Iteration 4551 / 12100) loss: 0.029485\n",
      "(Iteration 4561 / 12100) loss: 0.041859\n",
      "(Iteration 4571 / 12100) loss: 0.029465\n",
      "(Iteration 4581 / 12100) loss: 0.030049\n",
      "(Iteration 4591 / 12100) loss: 0.036550\n",
      "(Iteration 4601 / 12100) loss: 0.029115\n",
      "(Iteration 4611 / 12100) loss: 0.055671\n",
      "(Iteration 4621 / 12100) loss: 0.030654\n",
      "(Iteration 4631 / 12100) loss: 0.033151\n",
      "(Iteration 4641 / 12100) loss: 0.056608\n",
      "(Iteration 4651 / 12100) loss: 0.056584\n",
      "(Iteration 4661 / 12100) loss: 0.042542\n",
      "(Iteration 4671 / 12100) loss: 0.037685\n",
      "(Iteration 4681 / 12100) loss: 0.041181\n",
      "(Iteration 4691 / 12100) loss: 0.031900\n",
      "(Iteration 4701 / 12100) loss: 0.030682\n",
      "(Iteration 4711 / 12100) loss: 0.031128\n",
      "(Iteration 4721 / 12100) loss: 0.037261\n",
      "(Iteration 4731 / 12100) loss: 0.034186\n",
      "(Iteration 4741 / 12100) loss: 0.034443\n",
      "(Iteration 4751 / 12100) loss: 0.042319\n",
      "(Iteration 4761 / 12100) loss: 0.045850\n",
      "(Iteration 4771 / 12100) loss: 0.041118\n",
      "(Iteration 4781 / 12100) loss: 0.038597\n",
      "(Iteration 4791 / 12100) loss: 0.039529\n",
      "(Iteration 4801 / 12100) loss: 0.046456\n",
      "(Iteration 4811 / 12100) loss: 0.047498\n",
      "(Iteration 4821 / 12100) loss: 0.033230\n",
      "(Iteration 4831 / 12100) loss: 0.033954\n",
      "(Iteration 4841 / 12100) loss: 0.046564\n",
      "(Iteration 4851 / 12100) loss: 0.040818\n",
      "(Iteration 4861 / 12100) loss: 0.032149\n",
      "(Iteration 4871 / 12100) loss: 0.030320\n",
      "(Iteration 4881 / 12100) loss: 0.033784\n",
      "(Iteration 4891 / 12100) loss: 0.031283\n",
      "(Iteration 4901 / 12100) loss: 0.035085\n",
      "(Iteration 4911 / 12100) loss: 0.033074\n",
      "(Iteration 4921 / 12100) loss: 0.033769\n",
      "(Iteration 4931 / 12100) loss: 0.040386\n",
      "(Iteration 4941 / 12100) loss: 0.051588\n",
      "(Iteration 4951 / 12100) loss: 0.039904\n",
      "(Iteration 4961 / 12100) loss: 0.039494\n",
      "(Iteration 4971 / 12100) loss: 0.033138\n",
      "(Iteration 4981 / 12100) loss: 0.045039\n",
      "(Iteration 4991 / 12100) loss: 0.035318\n",
      "(Iteration 5001 / 12100) loss: 0.047702\n",
      "(Epoch 6 / 5000) train acc: 0.979339; val_acc: 0.852459\n",
      "F1: 0.8571428571428571 \t Precision 0.84375 \t recall 0.8709677419354839\n",
      "(Iteration 5011 / 12100) loss: 0.117671\n",
      "(Iteration 5021 / 12100) loss: 0.044460\n",
      "(Iteration 5031 / 12100) loss: 0.032976\n",
      "(Iteration 5041 / 12100) loss: 0.057685\n",
      "(Iteration 5051 / 12100) loss: 0.038614\n",
      "(Iteration 5061 / 12100) loss: 0.058181\n",
      "(Iteration 5071 / 12100) loss: 0.038945\n",
      "(Iteration 5081 / 12100) loss: 0.035818\n",
      "(Iteration 5091 / 12100) loss: 0.051777\n",
      "(Iteration 5101 / 12100) loss: 0.033945\n",
      "(Iteration 5111 / 12100) loss: 0.033431\n",
      "(Iteration 5121 / 12100) loss: 0.033951\n",
      "(Iteration 5131 / 12100) loss: 0.039954\n",
      "(Iteration 5141 / 12100) loss: 0.035343\n",
      "(Iteration 5151 / 12100) loss: 0.044396\n",
      "(Iteration 5161 / 12100) loss: 0.030260\n",
      "(Iteration 5171 / 12100) loss: 0.032905\n",
      "(Iteration 5181 / 12100) loss: 0.031731\n",
      "(Iteration 5191 / 12100) loss: 0.033183\n",
      "(Iteration 5201 / 12100) loss: 0.037982\n",
      "(Iteration 5211 / 12100) loss: 0.029775\n",
      "(Iteration 5221 / 12100) loss: 0.039593\n",
      "(Iteration 5231 / 12100) loss: 0.029840\n",
      "(Iteration 5241 / 12100) loss: 0.032293\n",
      "(Iteration 5251 / 12100) loss: 0.029024\n",
      "(Iteration 5261 / 12100) loss: 0.073172\n",
      "(Iteration 5271 / 12100) loss: 0.055817\n",
      "(Iteration 5281 / 12100) loss: 0.094133\n",
      "(Iteration 5291 / 12100) loss: 0.029884\n",
      "(Iteration 5301 / 12100) loss: 0.097445\n",
      "(Iteration 5311 / 12100) loss: 0.036892\n",
      "(Iteration 5321 / 12100) loss: 0.032596\n",
      "(Iteration 5331 / 12100) loss: 0.041931\n",
      "(Iteration 5341 / 12100) loss: 0.038307\n",
      "(Iteration 5351 / 12100) loss: 0.062605\n",
      "(Iteration 5361 / 12100) loss: 0.028986\n",
      "(Iteration 5371 / 12100) loss: 0.052511\n",
      "(Iteration 5381 / 12100) loss: 0.055655\n",
      "(Iteration 5391 / 12100) loss: 0.033211\n",
      "(Iteration 5401 / 12100) loss: 0.030245\n",
      "(Iteration 5411 / 12100) loss: 0.045094\n",
      "(Iteration 5421 / 12100) loss: 0.036487\n",
      "(Iteration 5431 / 12100) loss: 0.072864\n",
      "(Iteration 5441 / 12100) loss: 0.047363\n",
      "(Iteration 5451 / 12100) loss: 0.042057\n",
      "(Iteration 5461 / 12100) loss: 0.030174\n",
      "(Iteration 5471 / 12100) loss: 0.036212\n",
      "(Iteration 5481 / 12100) loss: 0.033266\n",
      "(Iteration 5491 / 12100) loss: 0.065950\n",
      "(Iteration 5501 / 12100) loss: 0.040858\n",
      "(Epoch 6 / 5000) train acc: 0.971074; val_acc: 0.868852\n",
      "F1: 0.8709677419354839 \t Precision 0.8709677419354839 \t recall 0.8709677419354839\n",
      "(Iteration 5511 / 12100) loss: 0.042373\n",
      "(Iteration 5521 / 12100) loss: 0.081259\n",
      "(Iteration 5531 / 12100) loss: 0.036921\n",
      "(Iteration 5541 / 12100) loss: 0.049620\n",
      "(Iteration 5551 / 12100) loss: 0.032440\n",
      "(Iteration 5561 / 12100) loss: 0.042432\n",
      "(Iteration 5571 / 12100) loss: 0.034909\n",
      "(Iteration 5581 / 12100) loss: 0.033279\n",
      "(Iteration 5591 / 12100) loss: 0.039967\n",
      "(Iteration 5601 / 12100) loss: 0.031939\n",
      "(Iteration 5611 / 12100) loss: 0.035833\n",
      "(Iteration 5621 / 12100) loss: 0.067391\n",
      "(Iteration 5631 / 12100) loss: 0.031936\n",
      "(Iteration 5641 / 12100) loss: 0.043439\n",
      "(Iteration 5651 / 12100) loss: 0.051291\n",
      "(Iteration 5661 / 12100) loss: 0.030916\n",
      "(Iteration 5671 / 12100) loss: 0.032641\n",
      "(Iteration 5681 / 12100) loss: 0.029813\n",
      "(Iteration 5691 / 12100) loss: 0.046232\n",
      "(Iteration 5701 / 12100) loss: 0.039223\n",
      "(Iteration 5711 / 12100) loss: 0.031890\n",
      "(Iteration 5721 / 12100) loss: 0.032902\n",
      "(Iteration 5731 / 12100) loss: 0.040731\n",
      "(Iteration 5741 / 12100) loss: 0.041073\n",
      "(Iteration 5751 / 12100) loss: 0.031135\n",
      "(Iteration 5761 / 12100) loss: 0.071910\n",
      "(Iteration 5771 / 12100) loss: 0.078771\n",
      "(Iteration 5781 / 12100) loss: 0.029494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 5791 / 12100) loss: 0.044208\n",
      "(Iteration 5801 / 12100) loss: 0.061463\n",
      "(Iteration 5811 / 12100) loss: 0.033298\n",
      "(Iteration 5821 / 12100) loss: 0.029663\n",
      "(Iteration 5831 / 12100) loss: 0.036033\n",
      "(Iteration 5841 / 12100) loss: 0.035482\n",
      "(Iteration 5851 / 12100) loss: 0.029306\n",
      "(Iteration 5861 / 12100) loss: 0.076212\n",
      "(Iteration 5871 / 12100) loss: 0.034448\n",
      "(Iteration 5881 / 12100) loss: 0.032367\n",
      "(Iteration 5891 / 12100) loss: 0.049598\n",
      "(Iteration 5901 / 12100) loss: 0.031096\n",
      "(Iteration 5911 / 12100) loss: 0.030251\n",
      "(Iteration 5921 / 12100) loss: 0.031424\n",
      "(Iteration 5931 / 12100) loss: 0.040649\n",
      "(Iteration 5941 / 12100) loss: 0.054138\n",
      "(Iteration 5951 / 12100) loss: 0.041685\n",
      "(Iteration 5961 / 12100) loss: 0.045449\n",
      "(Iteration 5971 / 12100) loss: 0.036049\n",
      "(Iteration 5981 / 12100) loss: 0.031599\n",
      "(Iteration 5991 / 12100) loss: 0.046352\n",
      "(Iteration 6001 / 12100) loss: 0.078766\n",
      "(Epoch 7 / 5000) train acc: 0.971074; val_acc: 0.836066\n",
      "F1: 0.84375 \t Precision 0.8181818181818182 \t recall 0.8709677419354839\n",
      "(Iteration 6011 / 12100) loss: 0.047034\n",
      "(Iteration 6021 / 12100) loss: 0.038980\n",
      "(Iteration 6031 / 12100) loss: 0.037736\n",
      "(Iteration 6041 / 12100) loss: 0.036624\n",
      "(Iteration 6051 / 12100) loss: 0.030100\n",
      "(Iteration 6061 / 12100) loss: 0.030930\n",
      "(Iteration 6071 / 12100) loss: 0.030126\n",
      "(Iteration 6081 / 12100) loss: 0.030710\n",
      "(Iteration 6091 / 12100) loss: 0.031602\n",
      "(Iteration 6101 / 12100) loss: 0.034537\n",
      "(Iteration 6111 / 12100) loss: 0.029753\n",
      "(Iteration 6121 / 12100) loss: 0.043752\n",
      "(Iteration 6131 / 12100) loss: 0.028805\n",
      "(Iteration 6141 / 12100) loss: 0.030751\n",
      "(Iteration 6151 / 12100) loss: 0.034674\n",
      "(Iteration 6161 / 12100) loss: 0.030599\n",
      "(Iteration 6171 / 12100) loss: 0.031667\n",
      "(Iteration 6181 / 12100) loss: 0.029565\n",
      "(Iteration 6191 / 12100) loss: 0.029555\n",
      "(Iteration 6201 / 12100) loss: 0.064462\n",
      "(Iteration 6211 / 12100) loss: 0.040737\n",
      "(Iteration 6221 / 12100) loss: 0.048949\n",
      "(Iteration 6231 / 12100) loss: 0.031896\n",
      "(Iteration 6241 / 12100) loss: 0.031814\n",
      "(Iteration 6251 / 12100) loss: 0.044516\n",
      "(Iteration 6261 / 12100) loss: 0.037047\n",
      "(Iteration 6271 / 12100) loss: 0.030992\n",
      "(Iteration 6281 / 12100) loss: 0.037006\n",
      "(Iteration 6291 / 12100) loss: 0.034677\n",
      "(Iteration 6301 / 12100) loss: 0.029724\n",
      "(Iteration 6311 / 12100) loss: 0.037004\n",
      "(Iteration 6321 / 12100) loss: 0.039780\n",
      "(Iteration 6331 / 12100) loss: 0.029262\n",
      "(Iteration 6341 / 12100) loss: 0.031840\n",
      "(Iteration 6351 / 12100) loss: 0.032667\n",
      "(Iteration 6361 / 12100) loss: 0.041610\n",
      "(Iteration 6371 / 12100) loss: 0.029878\n",
      "(Iteration 6381 / 12100) loss: 0.029043\n",
      "(Iteration 6391 / 12100) loss: 0.031126\n",
      "(Iteration 6401 / 12100) loss: 0.080900\n",
      "(Iteration 6411 / 12100) loss: 0.055008\n",
      "(Iteration 6421 / 12100) loss: 0.089671\n",
      "(Iteration 6431 / 12100) loss: 0.037335\n",
      "(Iteration 6441 / 12100) loss: 0.045990\n",
      "(Iteration 6451 / 12100) loss: 0.043632\n",
      "(Iteration 6461 / 12100) loss: 0.034313\n",
      "(Iteration 6471 / 12100) loss: 0.035552\n",
      "(Iteration 6481 / 12100) loss: 0.070550\n",
      "(Iteration 6491 / 12100) loss: 0.061586\n",
      "(Iteration 6501 / 12100) loss: 0.032110\n",
      "(Epoch 7 / 5000) train acc: 0.966942; val_acc: 0.885246\n",
      "F1: 0.8923076923076922 \t Precision 0.8529411764705882 \t recall 0.9354838709677419\n",
      "(Iteration 6511 / 12100) loss: 0.040966\n",
      "(Iteration 6521 / 12100) loss: 0.032173\n",
      "(Iteration 6531 / 12100) loss: 0.053353\n",
      "(Iteration 6541 / 12100) loss: 0.032575\n",
      "(Iteration 6551 / 12100) loss: 0.035788\n",
      "(Iteration 6561 / 12100) loss: 0.032483\n",
      "(Iteration 6571 / 12100) loss: 0.029650\n",
      "(Iteration 6581 / 12100) loss: 0.055815\n",
      "(Iteration 6591 / 12100) loss: 0.033872\n",
      "(Iteration 6601 / 12100) loss: 0.032104\n",
      "(Iteration 6611 / 12100) loss: 0.029868\n",
      "(Iteration 6621 / 12100) loss: 0.051561\n",
      "(Iteration 6631 / 12100) loss: 0.029668\n",
      "(Iteration 6641 / 12100) loss: 0.033954\n",
      "(Iteration 6651 / 12100) loss: 0.038703\n",
      "(Iteration 6661 / 12100) loss: 0.048473\n",
      "(Iteration 6671 / 12100) loss: 0.085192\n",
      "(Iteration 6681 / 12100) loss: 0.031863\n",
      "(Iteration 6691 / 12100) loss: 0.031051\n",
      "(Iteration 6701 / 12100) loss: 0.032255\n",
      "(Iteration 6711 / 12100) loss: 0.030948\n",
      "(Iteration 6721 / 12100) loss: 0.032628\n",
      "(Iteration 6731 / 12100) loss: 0.032934\n",
      "(Iteration 6741 / 12100) loss: 0.099679\n",
      "(Iteration 6751 / 12100) loss: 0.131478\n",
      "(Iteration 6761 / 12100) loss: 0.061492\n",
      "(Iteration 6771 / 12100) loss: 0.032919\n",
      "(Iteration 6781 / 12100) loss: 0.031816\n",
      "(Iteration 6791 / 12100) loss: 0.033601\n",
      "(Iteration 6801 / 12100) loss: 0.053599\n",
      "(Iteration 6811 / 12100) loss: 0.032725\n",
      "(Iteration 6821 / 12100) loss: 0.037111\n",
      "(Iteration 6831 / 12100) loss: 0.048669\n",
      "(Iteration 6841 / 12100) loss: 0.061930\n",
      "(Iteration 6851 / 12100) loss: 0.032015\n",
      "(Iteration 6861 / 12100) loss: 0.032757\n",
      "(Iteration 6871 / 12100) loss: 0.032719\n",
      "(Iteration 6881 / 12100) loss: 0.051166\n",
      "(Iteration 6891 / 12100) loss: 0.040318\n",
      "(Iteration 6901 / 12100) loss: 0.033237\n",
      "(Iteration 6911 / 12100) loss: 0.050351\n",
      "(Iteration 6921 / 12100) loss: 0.037464\n",
      "(Iteration 6931 / 12100) loss: 0.031627\n",
      "(Iteration 6941 / 12100) loss: 0.033670\n",
      "(Iteration 6951 / 12100) loss: 0.032085\n",
      "(Iteration 6961 / 12100) loss: 0.049587\n",
      "(Iteration 6971 / 12100) loss: 0.031924\n",
      "(Iteration 6981 / 12100) loss: 0.054620\n",
      "(Iteration 6991 / 12100) loss: 0.039157\n",
      "(Iteration 7001 / 12100) loss: 0.065510\n",
      "(Epoch 8 / 5000) train acc: 0.946281; val_acc: 0.868852\n",
      "F1: 0.8787878787878788 \t Precision 0.8285714285714286 \t recall 0.9354838709677419\n",
      "(Iteration 7011 / 12100) loss: 0.031578\n",
      "(Iteration 7021 / 12100) loss: 0.033962\n",
      "(Iteration 7031 / 12100) loss: 0.043117\n",
      "(Iteration 7041 / 12100) loss: 0.035107\n",
      "(Iteration 7051 / 12100) loss: 0.035431\n",
      "(Iteration 7061 / 12100) loss: 0.034269\n",
      "(Iteration 7071 / 12100) loss: 0.035894\n",
      "(Iteration 7081 / 12100) loss: 0.033896\n",
      "(Iteration 7091 / 12100) loss: 0.030585\n",
      "(Iteration 7101 / 12100) loss: 0.034581\n",
      "(Iteration 7111 / 12100) loss: 0.029674\n",
      "(Iteration 7121 / 12100) loss: 0.031167\n",
      "(Iteration 7131 / 12100) loss: 0.030529\n",
      "(Iteration 7141 / 12100) loss: 0.029844\n",
      "(Iteration 7151 / 12100) loss: 0.030876\n",
      "(Iteration 7161 / 12100) loss: 0.038527\n",
      "(Iteration 7171 / 12100) loss: 0.033055\n",
      "(Iteration 7181 / 12100) loss: 0.044505\n",
      "(Iteration 7191 / 12100) loss: 0.040409\n",
      "(Iteration 7201 / 12100) loss: 0.031346\n",
      "(Iteration 7211 / 12100) loss: 0.029666\n",
      "(Iteration 7221 / 12100) loss: 0.034119\n",
      "(Iteration 7231 / 12100) loss: 0.030605\n",
      "(Iteration 7241 / 12100) loss: 0.057942\n",
      "(Iteration 7251 / 12100) loss: 0.043658\n",
      "(Iteration 7261 / 12100) loss: 0.040260\n",
      "(Iteration 7271 / 12100) loss: 0.035502\n",
      "(Iteration 7281 / 12100) loss: 0.034558\n",
      "(Iteration 7291 / 12100) loss: 0.034606\n",
      "(Iteration 7301 / 12100) loss: 0.038698\n",
      "(Iteration 7311 / 12100) loss: 0.050277\n",
      "(Iteration 7321 / 12100) loss: 0.052984\n",
      "(Iteration 7331 / 12100) loss: 0.031180\n",
      "(Iteration 7341 / 12100) loss: 0.034310\n",
      "(Iteration 7351 / 12100) loss: 0.038105\n",
      "(Iteration 7361 / 12100) loss: 0.030641\n",
      "(Iteration 7371 / 12100) loss: 0.033531\n",
      "(Iteration 7381 / 12100) loss: 0.032111\n",
      "(Iteration 7391 / 12100) loss: 0.033915\n",
      "(Iteration 7401 / 12100) loss: 0.031973\n",
      "(Iteration 7411 / 12100) loss: 0.044877\n",
      "(Iteration 7421 / 12100) loss: 0.041415\n",
      "(Iteration 7431 / 12100) loss: 0.029523\n",
      "(Iteration 7441 / 12100) loss: 0.062878\n",
      "(Iteration 7451 / 12100) loss: 0.029442\n",
      "(Iteration 7461 / 12100) loss: 0.040456\n",
      "(Iteration 7471 / 12100) loss: 0.030855\n",
      "(Iteration 7481 / 12100) loss: 0.032589\n",
      "(Iteration 7491 / 12100) loss: 0.034425\n",
      "(Iteration 7501 / 12100) loss: 0.030490\n",
      "(Epoch 8 / 5000) train acc: 0.942149; val_acc: 0.885246\n",
      "F1: 0.8923076923076922 \t Precision 0.8529411764705882 \t recall 0.9354838709677419\n",
      "(Iteration 7511 / 12100) loss: 0.030789\n",
      "(Iteration 7521 / 12100) loss: 0.043023\n",
      "(Iteration 7531 / 12100) loss: 0.029290\n",
      "(Iteration 7541 / 12100) loss: 0.030854\n",
      "(Iteration 7551 / 12100) loss: 0.031231\n",
      "(Iteration 7561 / 12100) loss: 0.032731\n",
      "(Iteration 7571 / 12100) loss: 0.031586\n",
      "(Iteration 7581 / 12100) loss: 0.034339\n",
      "(Iteration 7591 / 12100) loss: 0.037390\n",
      "(Iteration 7601 / 12100) loss: 0.029328\n",
      "(Iteration 7611 / 12100) loss: 0.029118\n",
      "(Iteration 7621 / 12100) loss: 0.033642\n",
      "(Iteration 7631 / 12100) loss: 0.030939\n",
      "(Iteration 7641 / 12100) loss: 0.056812\n",
      "(Iteration 7651 / 12100) loss: 0.032469\n",
      "(Iteration 7661 / 12100) loss: 0.032748\n",
      "(Iteration 7671 / 12100) loss: 0.039390\n",
      "(Iteration 7681 / 12100) loss: 0.048777\n",
      "(Iteration 7691 / 12100) loss: 0.048246\n",
      "(Iteration 7701 / 12100) loss: 0.031218\n",
      "(Iteration 7711 / 12100) loss: 0.046425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 7721 / 12100) loss: 0.036575\n",
      "(Iteration 7731 / 12100) loss: 0.035039\n",
      "(Iteration 7741 / 12100) loss: 0.051712\n",
      "(Iteration 7751 / 12100) loss: 0.051466\n",
      "(Iteration 7761 / 12100) loss: 0.038048\n",
      "(Iteration 7771 / 12100) loss: 0.032121\n",
      "(Iteration 7781 / 12100) loss: 0.033759\n",
      "(Iteration 7791 / 12100) loss: 0.034612\n",
      "(Iteration 7801 / 12100) loss: 0.030434\n",
      "(Iteration 7811 / 12100) loss: 0.037044\n",
      "(Iteration 7821 / 12100) loss: 0.030333\n",
      "(Iteration 7831 / 12100) loss: 0.033688\n",
      "(Iteration 7841 / 12100) loss: 0.031639\n",
      "(Iteration 7851 / 12100) loss: 0.031911\n",
      "(Iteration 7861 / 12100) loss: 0.081792\n",
      "(Iteration 7871 / 12100) loss: 0.050829\n",
      "(Iteration 7881 / 12100) loss: 0.039753\n",
      "(Iteration 7891 / 12100) loss: 0.034685\n",
      "(Iteration 7901 / 12100) loss: 0.035790\n",
      "(Iteration 7911 / 12100) loss: 0.031555\n",
      "(Iteration 7921 / 12100) loss: 0.040675\n",
      "(Iteration 7931 / 12100) loss: 0.037826\n",
      "(Iteration 7941 / 12100) loss: 0.038032\n",
      "(Iteration 7951 / 12100) loss: 0.044962\n",
      "(Iteration 7961 / 12100) loss: 0.035277\n",
      "(Iteration 7971 / 12100) loss: 0.042198\n",
      "(Iteration 7981 / 12100) loss: 0.039188\n",
      "(Iteration 7991 / 12100) loss: 0.051911\n",
      "(Iteration 8001 / 12100) loss: 0.063034\n",
      "(Epoch 9 / 5000) train acc: 0.942149; val_acc: 0.885246\n",
      "F1: 0.8923076923076922 \t Precision 0.8529411764705882 \t recall 0.9354838709677419\n",
      "(Iteration 8011 / 12100) loss: 0.058021\n",
      "(Iteration 8021 / 12100) loss: 0.041831\n",
      "(Iteration 8031 / 12100) loss: 0.037043\n",
      "(Iteration 8041 / 12100) loss: 0.032617\n",
      "(Iteration 8051 / 12100) loss: 0.036262\n",
      "(Iteration 8061 / 12100) loss: 0.031691\n",
      "(Iteration 8071 / 12100) loss: 0.031549\n",
      "(Iteration 8081 / 12100) loss: 0.032416\n",
      "(Iteration 8091 / 12100) loss: 0.030462\n",
      "(Iteration 8101 / 12100) loss: 0.059708\n",
      "(Iteration 8111 / 12100) loss: 0.032161\n",
      "(Iteration 8121 / 12100) loss: 0.041205\n",
      "(Iteration 8131 / 12100) loss: 0.029967\n",
      "(Iteration 8141 / 12100) loss: 0.030678\n",
      "(Iteration 8151 / 12100) loss: 0.035010\n",
      "(Iteration 8161 / 12100) loss: 0.030342\n",
      "(Iteration 8171 / 12100) loss: 0.029221\n",
      "(Iteration 8181 / 12100) loss: 0.035727\n",
      "(Iteration 8191 / 12100) loss: 0.033703\n",
      "(Iteration 8201 / 12100) loss: 0.035595\n",
      "(Iteration 8211 / 12100) loss: 0.029088\n",
      "(Iteration 8221 / 12100) loss: 0.029017\n",
      "(Iteration 8231 / 12100) loss: 0.058171\n",
      "(Iteration 8241 / 12100) loss: 0.038286\n",
      "(Iteration 8251 / 12100) loss: 0.028528\n",
      "(Iteration 8261 / 12100) loss: 0.038931\n",
      "(Iteration 8271 / 12100) loss: 0.031858\n",
      "(Iteration 8281 / 12100) loss: 0.027823\n",
      "(Iteration 8291 / 12100) loss: 0.030989\n",
      "(Iteration 8301 / 12100) loss: 0.029454\n",
      "(Iteration 8311 / 12100) loss: 0.032050\n",
      "(Iteration 8321 / 12100) loss: 0.033613\n",
      "(Iteration 8331 / 12100) loss: 0.050998\n",
      "(Iteration 8341 / 12100) loss: 0.041951\n",
      "(Iteration 8351 / 12100) loss: 0.030241\n",
      "(Iteration 8361 / 12100) loss: 0.035875\n",
      "(Iteration 8371 / 12100) loss: 0.050501\n",
      "(Iteration 8381 / 12100) loss: 0.034776\n",
      "(Iteration 8391 / 12100) loss: 0.035560\n",
      "(Iteration 8401 / 12100) loss: 0.029745\n",
      "(Iteration 8411 / 12100) loss: 0.028675\n",
      "(Iteration 8421 / 12100) loss: 0.040387\n",
      "(Iteration 8431 / 12100) loss: 0.030628\n",
      "(Iteration 8441 / 12100) loss: 0.038656\n",
      "(Iteration 8451 / 12100) loss: 0.037719\n",
      "(Iteration 8461 / 12100) loss: 0.037450\n",
      "(Iteration 8471 / 12100) loss: 0.028806\n",
      "(Iteration 8481 / 12100) loss: 0.032259\n",
      "(Iteration 8491 / 12100) loss: 0.070884\n",
      "(Iteration 8501 / 12100) loss: 0.029412\n",
      "(Epoch 9 / 5000) train acc: 0.954545; val_acc: 0.852459\n",
      "F1: 0.8656716417910448 \t Precision 0.8055555555555556 \t recall 0.9354838709677419\n",
      "(Iteration 8511 / 12100) loss: 0.081546\n",
      "(Iteration 8521 / 12100) loss: 0.037537\n",
      "(Iteration 8531 / 12100) loss: 0.121434\n",
      "(Iteration 8541 / 12100) loss: 0.035863\n",
      "(Iteration 8551 / 12100) loss: 0.050870\n",
      "(Iteration 8561 / 12100) loss: 0.032102\n",
      "(Iteration 8571 / 12100) loss: 0.031349\n",
      "(Iteration 8581 / 12100) loss: 0.041604\n",
      "(Iteration 8591 / 12100) loss: 0.030789\n",
      "(Iteration 8601 / 12100) loss: 0.036158\n",
      "(Iteration 8611 / 12100) loss: 0.059650\n",
      "(Iteration 8621 / 12100) loss: 0.034704\n",
      "(Iteration 8631 / 12100) loss: 0.050846\n",
      "(Iteration 8641 / 12100) loss: 0.036460\n",
      "(Iteration 8651 / 12100) loss: 0.039687\n",
      "(Iteration 8661 / 12100) loss: 0.033408\n",
      "(Iteration 8671 / 12100) loss: 0.036376\n",
      "(Iteration 8681 / 12100) loss: 0.031352\n",
      "(Iteration 8691 / 12100) loss: 0.034406\n",
      "(Iteration 8701 / 12100) loss: 0.058317\n",
      "(Iteration 8711 / 12100) loss: 0.031929\n",
      "(Iteration 8721 / 12100) loss: 0.030770\n",
      "(Iteration 8731 / 12100) loss: 0.053560\n",
      "(Iteration 8741 / 12100) loss: 0.033968\n",
      "(Iteration 8751 / 12100) loss: 0.044375\n",
      "(Iteration 8761 / 12100) loss: 0.059640\n",
      "(Iteration 8771 / 12100) loss: 0.039578\n",
      "(Iteration 8781 / 12100) loss: 0.057837\n",
      "(Iteration 8791 / 12100) loss: 0.042333\n",
      "(Iteration 8801 / 12100) loss: 0.078957\n",
      "(Iteration 8811 / 12100) loss: 0.035826\n",
      "(Iteration 8821 / 12100) loss: 0.052999\n",
      "(Iteration 8831 / 12100) loss: 0.046777\n",
      "(Iteration 8841 / 12100) loss: 0.039384\n",
      "(Iteration 8851 / 12100) loss: 0.032935\n",
      "(Iteration 8861 / 12100) loss: 0.033466\n",
      "(Iteration 8871 / 12100) loss: 0.033619\n",
      "(Iteration 8881 / 12100) loss: 0.045465\n",
      "(Iteration 8891 / 12100) loss: 0.031262\n",
      "(Iteration 8901 / 12100) loss: 0.031612\n",
      "(Iteration 8911 / 12100) loss: 0.038220\n",
      "(Iteration 8921 / 12100) loss: 0.032530\n",
      "(Iteration 8931 / 12100) loss: 0.031897\n",
      "(Iteration 8941 / 12100) loss: 0.033472\n",
      "(Iteration 8951 / 12100) loss: 0.029789\n",
      "(Iteration 8961 / 12100) loss: 0.045170\n",
      "(Iteration 8971 / 12100) loss: 0.031840\n",
      "(Iteration 8981 / 12100) loss: 0.031254\n",
      "(Iteration 8991 / 12100) loss: 0.035580\n",
      "(Iteration 9001 / 12100) loss: 0.029977\n",
      "(Epoch 10 / 5000) train acc: 0.962810; val_acc: 0.885246\n",
      "F1: 0.8923076923076922 \t Precision 0.8529411764705882 \t recall 0.9354838709677419\n",
      "(Iteration 9011 / 12100) loss: 0.068307\n",
      "(Iteration 9021 / 12100) loss: 0.053126\n",
      "(Iteration 9031 / 12100) loss: 0.045389\n",
      "(Iteration 9041 / 12100) loss: 0.108076\n",
      "(Iteration 9051 / 12100) loss: 0.030222\n",
      "(Iteration 9061 / 12100) loss: 0.034737\n",
      "(Iteration 9071 / 12100) loss: 0.080604\n",
      "(Iteration 9081 / 12100) loss: 0.033490\n",
      "(Iteration 9091 / 12100) loss: 0.029486\n",
      "(Iteration 9101 / 12100) loss: 0.032776\n",
      "(Iteration 9111 / 12100) loss: 0.074179\n",
      "(Iteration 9121 / 12100) loss: 0.031274\n",
      "(Iteration 9131 / 12100) loss: 0.033039\n",
      "(Iteration 9141 / 12100) loss: 0.035201\n",
      "(Iteration 9151 / 12100) loss: 0.030070\n",
      "(Iteration 9161 / 12100) loss: 0.031719\n",
      "(Iteration 9171 / 12100) loss: 0.045667\n",
      "(Iteration 9181 / 12100) loss: 0.031162\n",
      "(Iteration 9191 / 12100) loss: 0.041933\n",
      "(Iteration 9201 / 12100) loss: 0.029225\n",
      "(Iteration 9211 / 12100) loss: 0.029790\n",
      "(Iteration 9221 / 12100) loss: 0.028865\n",
      "(Iteration 9231 / 12100) loss: 0.028935\n",
      "(Iteration 9241 / 12100) loss: 0.029376\n",
      "(Iteration 9251 / 12100) loss: 0.028102\n",
      "(Iteration 9261 / 12100) loss: 0.032903\n",
      "(Iteration 9271 / 12100) loss: 0.030228\n",
      "(Iteration 9281 / 12100) loss: 0.046095\n",
      "(Iteration 9291 / 12100) loss: 0.027971\n",
      "(Iteration 9301 / 12100) loss: 0.032482\n",
      "(Iteration 9311 / 12100) loss: 0.027823\n",
      "(Iteration 9321 / 12100) loss: 0.033717\n",
      "(Iteration 9331 / 12100) loss: 0.028029\n",
      "(Iteration 9341 / 12100) loss: 0.028524\n",
      "(Iteration 9351 / 12100) loss: 0.028528\n",
      "(Iteration 9361 / 12100) loss: 0.030239\n",
      "(Iteration 9371 / 12100) loss: 0.029915\n",
      "(Iteration 9381 / 12100) loss: 0.029396\n",
      "(Iteration 9391 / 12100) loss: 0.029781\n",
      "(Iteration 9401 / 12100) loss: 0.027707\n",
      "(Iteration 9411 / 12100) loss: 0.040912\n",
      "(Iteration 9421 / 12100) loss: 0.030678\n",
      "(Iteration 9431 / 12100) loss: 0.028345\n",
      "(Iteration 9441 / 12100) loss: 0.035978\n",
      "(Iteration 9451 / 12100) loss: 0.036094\n",
      "(Iteration 9461 / 12100) loss: 0.064212\n",
      "(Iteration 9471 / 12100) loss: 0.030385\n",
      "(Iteration 9481 / 12100) loss: 0.036309\n",
      "(Iteration 9491 / 12100) loss: 0.040370\n",
      "(Iteration 9501 / 12100) loss: 0.040356\n",
      "(Epoch 10 / 5000) train acc: 0.950413; val_acc: 0.885246\n",
      "F1: 0.8923076923076922 \t Precision 0.8529411764705882 \t recall 0.9354838709677419\n",
      "(Iteration 9511 / 12100) loss: 0.033575\n",
      "(Iteration 9521 / 12100) loss: 0.037469\n",
      "(Iteration 9531 / 12100) loss: 0.065442\n",
      "(Iteration 9541 / 12100) loss: 0.069356\n",
      "(Iteration 9551 / 12100) loss: 0.038986\n",
      "(Iteration 9561 / 12100) loss: 0.060506\n",
      "(Iteration 9571 / 12100) loss: 0.031678\n",
      "(Iteration 9581 / 12100) loss: 0.032731\n",
      "(Iteration 9591 / 12100) loss: 0.076023\n",
      "(Iteration 9601 / 12100) loss: 0.037661\n",
      "(Iteration 9611 / 12100) loss: 0.034854\n",
      "(Iteration 9621 / 12100) loss: 0.035955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 9631 / 12100) loss: 0.036561\n",
      "(Iteration 9641 / 12100) loss: 0.034963\n",
      "(Iteration 9651 / 12100) loss: 0.099825\n",
      "(Iteration 9661 / 12100) loss: 0.039213\n",
      "(Iteration 9671 / 12100) loss: 0.056001\n",
      "(Iteration 9681 / 12100) loss: 0.032279\n",
      "(Iteration 9691 / 12100) loss: 0.030943\n",
      "(Iteration 9701 / 12100) loss: 0.063115\n",
      "(Iteration 9711 / 12100) loss: 0.033527\n",
      "(Iteration 9721 / 12100) loss: 0.030690\n",
      "(Iteration 9731 / 12100) loss: 0.060263\n",
      "(Iteration 9741 / 12100) loss: 0.071146\n",
      "(Iteration 9751 / 12100) loss: 0.031570\n",
      "(Iteration 9761 / 12100) loss: 0.031967\n",
      "(Iteration 9771 / 12100) loss: 0.058168\n",
      "(Iteration 9781 / 12100) loss: 0.038244\n",
      "(Iteration 9791 / 12100) loss: 0.033012\n",
      "(Iteration 9801 / 12100) loss: 0.049848\n",
      "(Iteration 9811 / 12100) loss: 0.036800\n",
      "(Iteration 9821 / 12100) loss: 0.031009\n",
      "(Iteration 9831 / 12100) loss: 0.033195\n",
      "(Iteration 9841 / 12100) loss: 0.032753\n",
      "(Iteration 9851 / 12100) loss: 0.091005\n",
      "(Iteration 9861 / 12100) loss: 0.051784\n",
      "(Iteration 9871 / 12100) loss: 0.052538\n",
      "(Iteration 9881 / 12100) loss: 0.048947\n",
      "(Iteration 9891 / 12100) loss: 0.058429\n",
      "(Iteration 9901 / 12100) loss: 0.053679\n",
      "(Iteration 9911 / 12100) loss: 0.045545\n",
      "(Iteration 9921 / 12100) loss: 0.032433\n",
      "(Iteration 9931 / 12100) loss: 0.032954\n",
      "(Iteration 9941 / 12100) loss: 0.034308\n",
      "(Iteration 9951 / 12100) loss: 0.036379\n",
      "(Iteration 9961 / 12100) loss: 0.037074\n",
      "(Iteration 9971 / 12100) loss: 0.057584\n",
      "(Iteration 9981 / 12100) loss: 0.038499\n",
      "(Iteration 9991 / 12100) loss: 0.032691\n",
      "(Iteration 10001 / 12100) loss: 0.031371\n",
      "(Epoch 11 / 5000) train acc: 0.942149; val_acc: 0.885246\n",
      "F1: 0.8923076923076922 \t Precision 0.8529411764705882 \t recall 0.9354838709677419\n",
      "(Iteration 10011 / 12100) loss: 0.031003\n",
      "(Iteration 10021 / 12100) loss: 0.031226\n",
      "(Iteration 10031 / 12100) loss: 0.035508\n",
      "(Iteration 10041 / 12100) loss: 0.034128\n",
      "(Iteration 10051 / 12100) loss: 0.039074\n",
      "(Iteration 10061 / 12100) loss: 0.038999\n",
      "(Iteration 10071 / 12100) loss: 0.051085\n",
      "(Iteration 10081 / 12100) loss: 0.036818\n",
      "(Iteration 10091 / 12100) loss: 0.031619\n",
      "(Iteration 10101 / 12100) loss: 0.031894\n",
      "(Iteration 10111 / 12100) loss: 0.032917\n",
      "(Iteration 10121 / 12100) loss: 0.032061\n",
      "(Iteration 10131 / 12100) loss: 0.038962\n",
      "(Iteration 10141 / 12100) loss: 0.032833\n",
      "(Iteration 10151 / 12100) loss: 0.030548\n",
      "(Iteration 10161 / 12100) loss: 0.033062\n",
      "(Iteration 10171 / 12100) loss: 0.039744\n",
      "(Iteration 10181 / 12100) loss: 0.040550\n",
      "(Iteration 10191 / 12100) loss: 0.033313\n",
      "(Iteration 10201 / 12100) loss: 0.036288\n",
      "(Iteration 10211 / 12100) loss: 0.035181\n",
      "(Iteration 10221 / 12100) loss: 0.032807\n",
      "(Iteration 10231 / 12100) loss: 0.033862\n",
      "(Iteration 10241 / 12100) loss: 0.040305\n",
      "(Iteration 10251 / 12100) loss: 0.033603\n",
      "(Iteration 10261 / 12100) loss: 0.030711\n",
      "(Iteration 10271 / 12100) loss: 0.062733\n",
      "(Iteration 10281 / 12100) loss: 0.040706\n",
      "(Iteration 10291 / 12100) loss: 0.040337\n",
      "(Iteration 10301 / 12100) loss: 0.036141\n",
      "(Iteration 10311 / 12100) loss: 0.060631\n",
      "(Iteration 10321 / 12100) loss: 0.094056\n",
      "(Iteration 10331 / 12100) loss: 0.138976\n",
      "(Iteration 10341 / 12100) loss: 0.031842\n",
      "(Iteration 10351 / 12100) loss: 0.035169\n",
      "(Iteration 10361 / 12100) loss: 0.032458\n",
      "(Iteration 10371 / 12100) loss: 0.035339\n",
      "(Iteration 10381 / 12100) loss: 0.033821\n",
      "(Iteration 10391 / 12100) loss: 0.037160\n",
      "(Iteration 10401 / 12100) loss: 0.062595\n",
      "(Iteration 10411 / 12100) loss: 0.045627\n",
      "(Iteration 10421 / 12100) loss: 0.033964\n",
      "(Iteration 10431 / 12100) loss: 0.032264\n",
      "(Iteration 10441 / 12100) loss: 0.037648\n",
      "(Iteration 10451 / 12100) loss: 0.032887\n",
      "(Iteration 10461 / 12100) loss: 0.032254\n",
      "(Iteration 10471 / 12100) loss: 0.085396\n",
      "(Iteration 10481 / 12100) loss: 0.037904\n",
      "(Iteration 10491 / 12100) loss: 0.039215\n",
      "(Iteration 10501 / 12100) loss: 0.031838\n",
      "(Epoch 11 / 5000) train acc: 0.950413; val_acc: 0.885246\n",
      "F1: 0.8923076923076922 \t Precision 0.8529411764705882 \t recall 0.9354838709677419\n",
      "(Iteration 10511 / 12100) loss: 0.034315\n",
      "(Iteration 10521 / 12100) loss: 0.040180\n",
      "(Iteration 10531 / 12100) loss: 0.055734\n",
      "(Iteration 10541 / 12100) loss: 0.045993\n",
      "(Iteration 10551 / 12100) loss: 0.047041\n",
      "(Iteration 10561 / 12100) loss: 0.036003\n",
      "(Iteration 10571 / 12100) loss: 0.036783\n",
      "(Iteration 10581 / 12100) loss: 0.040395\n",
      "(Iteration 10591 / 12100) loss: 0.031139\n",
      "(Iteration 10601 / 12100) loss: 0.034182\n",
      "(Iteration 10611 / 12100) loss: 0.035483\n",
      "(Iteration 10621 / 12100) loss: 0.031340\n",
      "(Iteration 10631 / 12100) loss: 0.034690\n",
      "(Iteration 10641 / 12100) loss: 0.032981\n",
      "(Iteration 10651 / 12100) loss: 0.031821\n",
      "(Iteration 10661 / 12100) loss: 0.031089\n",
      "(Iteration 10671 / 12100) loss: 0.031236\n",
      "(Iteration 10681 / 12100) loss: 0.031212\n",
      "(Iteration 10691 / 12100) loss: 0.099317\n",
      "(Iteration 10701 / 12100) loss: 0.031786\n",
      "(Iteration 10711 / 12100) loss: 0.031292\n",
      "(Iteration 10721 / 12100) loss: 0.032874\n",
      "(Iteration 10731 / 12100) loss: 0.039571\n",
      "(Iteration 10741 / 12100) loss: 0.031260\n",
      "(Iteration 10751 / 12100) loss: 0.031954\n",
      "(Iteration 10761 / 12100) loss: 0.041993\n",
      "(Iteration 10771 / 12100) loss: 0.032574\n",
      "(Iteration 10781 / 12100) loss: 0.033890\n",
      "(Iteration 10791 / 12100) loss: 0.033015\n",
      "(Iteration 10801 / 12100) loss: 0.031550\n",
      "(Iteration 10811 / 12100) loss: 0.029454\n",
      "(Iteration 10821 / 12100) loss: 0.031545\n",
      "(Iteration 10831 / 12100) loss: 0.030505\n",
      "(Iteration 10841 / 12100) loss: 0.028949\n",
      "(Iteration 10851 / 12100) loss: 0.028395\n",
      "(Iteration 10861 / 12100) loss: 0.038119\n",
      "(Iteration 10871 / 12100) loss: 0.043473\n",
      "(Iteration 10881 / 12100) loss: 0.039133\n",
      "(Iteration 10891 / 12100) loss: 0.036245\n",
      "(Iteration 10901 / 12100) loss: 0.029485\n",
      "(Iteration 10911 / 12100) loss: 0.028529\n",
      "(Iteration 10921 / 12100) loss: 0.029018\n",
      "(Iteration 10931 / 12100) loss: 0.028259\n",
      "(Iteration 10941 / 12100) loss: 0.030012\n",
      "(Iteration 10951 / 12100) loss: 0.028606\n",
      "(Iteration 10961 / 12100) loss: 0.028205\n",
      "(Iteration 10971 / 12100) loss: 0.031898\n",
      "(Iteration 10981 / 12100) loss: 0.029830\n",
      "(Iteration 10991 / 12100) loss: 0.030514\n",
      "(Iteration 11001 / 12100) loss: 0.045672\n",
      "(Epoch 12 / 5000) train acc: 0.958678; val_acc: 0.885246\n",
      "F1: 0.8923076923076922 \t Precision 0.8529411764705882 \t recall 0.9354838709677419\n",
      "(Iteration 11011 / 12100) loss: 0.029004\n",
      "(Iteration 11021 / 12100) loss: 0.027326\n",
      "(Iteration 11031 / 12100) loss: 0.030721\n",
      "(Iteration 11041 / 12100) loss: 0.030988\n",
      "(Iteration 11051 / 12100) loss: 0.027969\n",
      "(Iteration 11061 / 12100) loss: 0.028421\n",
      "(Iteration 11071 / 12100) loss: 0.032983\n",
      "(Iteration 11081 / 12100) loss: 0.029305\n",
      "(Iteration 11091 / 12100) loss: 0.027081\n",
      "(Iteration 11101 / 12100) loss: 0.068258\n",
      "(Iteration 11111 / 12100) loss: 0.030400\n",
      "(Iteration 11121 / 12100) loss: 0.030899\n",
      "(Iteration 11131 / 12100) loss: 0.034400\n",
      "(Iteration 11141 / 12100) loss: 0.031782\n",
      "(Iteration 11151 / 12100) loss: 0.028159\n",
      "(Iteration 11161 / 12100) loss: 0.064823\n",
      "(Iteration 11171 / 12100) loss: 0.030527\n",
      "(Iteration 11181 / 12100) loss: 0.029503\n",
      "(Iteration 11191 / 12100) loss: 0.035827\n",
      "(Iteration 11201 / 12100) loss: 0.030837\n",
      "(Iteration 11211 / 12100) loss: 0.034784\n",
      "(Iteration 11221 / 12100) loss: 0.083652\n",
      "(Iteration 11231 / 12100) loss: 0.029065\n",
      "(Iteration 11241 / 12100) loss: 0.029355\n",
      "(Iteration 11251 / 12100) loss: 0.030916\n",
      "(Iteration 11261 / 12100) loss: 0.034119\n",
      "(Iteration 11271 / 12100) loss: 0.031311\n",
      "(Iteration 11281 / 12100) loss: 0.029836\n",
      "(Iteration 11291 / 12100) loss: 0.065507\n",
      "(Iteration 11301 / 12100) loss: 0.036304\n",
      "(Iteration 11311 / 12100) loss: 0.045469\n",
      "(Iteration 11321 / 12100) loss: 0.044350\n",
      "(Iteration 11331 / 12100) loss: 0.044649\n",
      "(Iteration 11341 / 12100) loss: 0.030131\n",
      "(Iteration 11351 / 12100) loss: 0.032217\n",
      "(Iteration 11361 / 12100) loss: 0.047568\n",
      "(Iteration 11371 / 12100) loss: 0.032098\n",
      "(Iteration 11381 / 12100) loss: 0.029324\n",
      "(Iteration 11391 / 12100) loss: 0.030612\n",
      "(Iteration 11401 / 12100) loss: 0.033140\n",
      "(Iteration 11411 / 12100) loss: 0.029679\n",
      "(Iteration 11421 / 12100) loss: 0.040508\n",
      "(Iteration 11431 / 12100) loss: 0.049118\n",
      "(Iteration 11441 / 12100) loss: 0.029718\n",
      "(Iteration 11451 / 12100) loss: 0.032018\n",
      "(Iteration 11461 / 12100) loss: 0.051037\n",
      "(Iteration 11471 / 12100) loss: 0.030220\n",
      "(Iteration 11481 / 12100) loss: 0.030408\n",
      "(Iteration 11491 / 12100) loss: 0.037990\n",
      "(Iteration 11501 / 12100) loss: 0.087030\n",
      "(Epoch 12 / 5000) train acc: 0.950413; val_acc: 0.868852\n",
      "F1: 0.875 \t Precision 0.8484848484848485 \t recall 0.9032258064516129\n",
      "(Iteration 11511 / 12100) loss: 0.033914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 11521 / 12100) loss: 0.049261\n",
      "(Iteration 11531 / 12100) loss: 0.033242\n",
      "(Iteration 11541 / 12100) loss: 0.050548\n",
      "(Iteration 11551 / 12100) loss: 0.052968\n",
      "(Iteration 11561 / 12100) loss: 0.034973\n",
      "(Iteration 11571 / 12100) loss: 0.040074\n",
      "(Iteration 11581 / 12100) loss: 0.034080\n",
      "(Iteration 11591 / 12100) loss: 0.032064\n",
      "(Iteration 11601 / 12100) loss: 0.032073\n",
      "(Iteration 11611 / 12100) loss: 0.031215\n",
      "(Iteration 11621 / 12100) loss: 0.041533\n",
      "(Iteration 11631 / 12100) loss: 0.056616\n",
      "(Iteration 11641 / 12100) loss: 0.031187\n",
      "(Iteration 11651 / 12100) loss: 0.038356\n",
      "(Iteration 11661 / 12100) loss: 0.052399\n",
      "(Iteration 11671 / 12100) loss: 0.030506\n",
      "(Iteration 11681 / 12100) loss: 0.032544\n",
      "(Iteration 11691 / 12100) loss: 0.030286\n",
      "(Iteration 11701 / 12100) loss: 0.033426\n",
      "(Iteration 11711 / 12100) loss: 0.062937\n",
      "(Iteration 11721 / 12100) loss: 0.103440\n",
      "(Iteration 11731 / 12100) loss: 0.038394\n",
      "(Iteration 11741 / 12100) loss: 0.030652\n",
      "(Iteration 11751 / 12100) loss: 0.082001\n",
      "(Iteration 11761 / 12100) loss: 0.038541\n",
      "(Iteration 11771 / 12100) loss: 0.041235\n",
      "(Iteration 11781 / 12100) loss: 0.033229\n",
      "(Iteration 11791 / 12100) loss: 0.036029\n",
      "(Iteration 11801 / 12100) loss: 0.030146\n",
      "(Iteration 11811 / 12100) loss: 0.031384\n",
      "(Iteration 11821 / 12100) loss: 0.040161\n",
      "(Iteration 11831 / 12100) loss: 0.037249\n",
      "(Iteration 11841 / 12100) loss: 0.065076\n",
      "(Iteration 11851 / 12100) loss: 0.032628\n",
      "(Iteration 11861 / 12100) loss: 0.031805\n",
      "(Iteration 11871 / 12100) loss: 0.032170\n",
      "(Iteration 11881 / 12100) loss: 0.030036\n",
      "(Iteration 11891 / 12100) loss: 0.032335\n",
      "(Iteration 11901 / 12100) loss: 0.029982\n",
      "(Iteration 11911 / 12100) loss: 0.046015\n",
      "(Iteration 11921 / 12100) loss: 0.062096\n",
      "(Iteration 11931 / 12100) loss: 0.031278\n",
      "(Iteration 11941 / 12100) loss: 0.032222\n",
      "(Iteration 11951 / 12100) loss: 0.055048\n",
      "(Iteration 11961 / 12100) loss: 0.046190\n",
      "(Iteration 11971 / 12100) loss: 0.035335\n",
      "(Iteration 11981 / 12100) loss: 0.032233\n",
      "(Iteration 11991 / 12100) loss: 0.031303\n",
      "(Iteration 12001 / 12100) loss: 0.032952\n",
      "(Epoch 13 / 5000) train acc: 0.946281; val_acc: 0.868852\n",
      "F1: 0.875 \t Precision 0.8484848484848485 \t recall 0.9032258064516129\n",
      "(Iteration 12011 / 12100) loss: 0.071335\n",
      "(Iteration 12021 / 12100) loss: 0.048976\n",
      "(Iteration 12031 / 12100) loss: 0.052981\n",
      "(Iteration 12041 / 12100) loss: 0.038271\n",
      "(Iteration 12051 / 12100) loss: 0.036562\n",
      "(Iteration 12061 / 12100) loss: 0.030830\n",
      "(Iteration 12071 / 12100) loss: 0.031244\n",
      "(Iteration 12081 / 12100) loss: 0.046853\n",
      "(Iteration 12091 / 12100) loss: 0.031498\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FullyConnectedNet([300, 200], input_dim=13, num_classes=2, dropout=0.5, reg=0.001)\n",
    "\n",
    "\n",
    "trainer = trainer.Trainer(model, data, num_epochs=5000)\n",
    "trainer.train()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(trainer.get_losshistory(), 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(trainer.get_train_acc_history(), '-o')\n",
    "plt.plot(trainer.get_val_acc_history(), '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
